Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Gyorgyi2001,
abstract = {In this article we review the framework for spontaneous replica symmetry breaking. Subsequently that is applied to the example of the statistical mechanical description of the storage properties of a McCulloch-Pitts neuron, i.e., simple perceptron. It is shown that in the neuron problem, the general formula that is at the core of all problems admitting Parisi's replica symmetry breaking ansatz with a one-component order parameter appears. The details of Parisi's method are reviewed extensively, with regard to the wide range of systems where the method may be applied. Parisi's partial differential equation and related differential equations are discussed, and the Green function technique is introduced for the calculation of replica averages, the key to determining the averages of physical quantities. The Green function of the Fokker-Planck equation due to Sompolinsky turns out to play the role of the statistical mechanical Green function in the graph rules for replica correlators. The subsequently obtained graph rules involve only tree graphs, as appropriate for a mean-field-like model. The lowest order Ward-Takahashi identity is recovered analytically and shown to lead to the Goldstone modes in continuous replica symmetry breaking phases. The need for a replica symmetry breaking theory in the storage problem of the neuron has arisen due to the thermodynamical instability of formerly given solutions. Variational forms for the neuron's free energy are derived in terms of the order parameter function x(q), for different prior distribution of synapses. Analytically in the high temperature limit and numerically in generic cases various phases are identified, among them is one similar to the Parisi phase in long-range interaction spin glasses. Extensive quantities like the error per pattern change slightly with respect to the known unstable solutions, but there is a significant difference in the distribution of non-extensive quantities like the synaptic overlaps and the pattern storage stability parameter. A simulation result is also reviewed and compared with the prediction of the theory. {\textcopyright} 2001 Elsevier Science B.V.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0010025},
author = {Gy{\"{o}}rgyi, G.},
doi = {10.1016/S0370-1573(00)00073-9},
eprint = {0010025},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0010025.pdf:pdf},
isbn = {0370-1573},
issn = {03701573},
journal = {Physics Report},
keywords = {07.05.Mh,61.43. - j,75.10Nr,84.35.+i,Neural networks,Pattern storage,Replica symmetry breaking,Spin glasses},
number = {4-5},
pages = {263--392},
primaryClass = {cond-mat},
title = {{Techniques of replica symmetry breaking and the storage problem of the McCulloch-Pitts neuron}},
volume = {342},
year = {2001}
}
@article{Bandeira2018,
abstract = {In these notes we describe heuristics to predict computational-to-statistical gaps in certain statistical problems. These are regimes in which the underlying statistical problem is information-theoretically possible although no efficient algorithm exists, rendering the problem essentially unsolvable for large instances. The methods we describe here are based on mature, albeit non-rigorous, tools from statistical physics. These notes are based on a lecture series given by the authors at the Courant Institute of Mathematical Sciences in New York City, on May 16th, 2017.},
archivePrefix = {arXiv},
arxivId = {1803.11132},
author = {Bandeira, Afonso S. and Perry, Amelia and Wein, Alexander S.},
eprint = {1803.11132},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1803.11132.pdf:pdf},
pages = {1--22},
title = {{Notes on computational-to-statistical gaps: predictions using statistical physics}},
url = {http://arxiv.org/abs/1803.11132},
year = {2018}
}
@article{Krauth1989,
author = {Krauth, W and Mezard, M},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/89{\_}KM{\_}JDP.pdf:pdf},
journal = {J. Phys (France)},
pages = {3057--3066},
title = {{Storage capacity of memory networks with binary coupling}},
volume = {50},
year = {1989}
}
@book{Neri2010,
abstract = {Statistical mechanics of finitely-connected systems is the study of spin models on random graphs. The methods developed to study spin models on graphs find their origins in spin-glass theory and are applicable to the description of emergent phenomena in coding theory, computer science, statistical inference in molecular biology, etc. The cornerstone of spin-glass theory is the Sherrington-Kirkpatrick model. This is a fully-connected model of spin variables interacting through random interactions drawn from a Gaussian distribution. The Sherrington-Kirkpatrick model is universal in the sense that the phase diagram of every fully-connected model with a finite variance is the same as the phase diagram of the Sherrington-Kirkpatrick model. When the variance of the distribution of couplings is infinitely large we speak of the L{\'{e}}vy spin glass. We show how this fully-connected model L{\'{e}}vy spin glass can be decomposed in a finitely-connected backbone of strong bonds interacting with the background of weak bonds. On the basis of this decomposition we derive the phase diagram of the L{\'{e}}vy spin glass. We show how to extend the tools of statistical mechanics of finitely-connected systems to non-equilibrium models where the steady state is not known a priori. Our aim is the development of an efficient and accurate message-passing algorithm for the study of non-equilibrium models, analogous to the belief-propagation algorithm for equilibrium models. We construct such an algorithm through the analysis of some simple models: models of binary spins evolving through Glauber dynamics on partially asymmetric graphs which are local tree like. Non- equilibrium systems evolving on a complex network are e.g. neurons in biological organisms, the spreading of epidemics and traffic networks. We discuss the influence of the graph structure on the retrieval properties of a neural network. Spin models on graphs can be used to study the performance of low-density parity-check codes. Low-density parity-check codes are efficient codes reaching the Shannon limit for reliable communication over memoryless symmetric channels. An important question is whether the low-density parity-check codes can also reach the Shannon limit in asymmetric channels are finite-state Markov channels. Finite v vi state Markov-channels or channels with memory in the noise. These channels are more realistic models for optical and wireless communication. For asymmetric channels the code performance depends on the codeword sent while for channels with memory the performance depends on the state during the use of the channel. The decoding of low-density parity-check code for channels with memory is realized through a message passing algorithm on a small-world hypergraph.},
author = {Neri, Izaak},
booktitle = {Status: Published},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/phd.pdf:pdf},
isbn = {9789086493456},
number = {June},
title = {{Statistical mechanics of spin models on graphs.}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+mechanics+of+spin+models+on+graphs{\#}1},
year = {2010}
}
@article{Poncet2015,
author = {Poncet, Alexis},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Links between Jamming, Glasses and Constraint Satisfaction Problems.pdf:pdf},
number = {January},
pages = {1--18},
title = {{Links between Jamming , Glasses and Constraint Satisfaction Problems}},
year = {2015}
}
@article{Diffusion1995,
author = {Diffusion, Atomic},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/48. Weight space structure and representations.pdf:pdf},
number = {12},
pages = {2364--2367},
title = {{Physical review letters 18}},
volume = {75},
year = {1995}
}
@article{Kabashima1994a,
author = {Kabashima, Y.},
doi = {10.1088/0305-4470/27/6/017},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/53. Perfect loss of generalization due to noise in K=2 parity machines.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {6},
pages = {1917--1927},
title = {{Perfect loss of generalization due to noise in K=2 parity machines}},
volume = {27},
year = {1994}
}
@article{Pennington2017a,
abstract = {In the nematode, Caenorhabditis elegans, VA and VB motor neurons arise from a common precursor cell but adopt different morphologies and synapse with separate sets of interneurons in the ventral nerve cord. A mutation that inactivates the unc-4 homeodomain gene causes VA motor neurons to assume the VB pattern of synaptic input while retaining normal axonal polarity and output; the disconnection of VA motor neurons from their usual presynaptic partners blocks backward locomotion. We show that expression of a functional unc-4-beta-galactosidase chimeric protein in VA motor neurons restores wild-type movement to an unc-4 mutant. We propose that unc-4 controls a differentiated characteristic of the VA motor neurons that distinguishes them from their VB sisters, thus dictating recognition by the appropriate interneurons. Our results show that synaptic choice can be controlled at the level of transcription in the post-synaptic neuron and identify a homeoprotein that defines a subset of cell-specific traits required for this choice.},
author = {Pennington, Jeffrey and Brain, Google and Worah, Pratik},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Pennington - nonlinear-random-matrix-theory-for-deep-learning.pdf:pdf},
isbn = {0950-1991 (Print)},
issn = {0950-1991},
journal = {Neural Information Processing Systems},
number = {31},
pmid = {7555714},
title = {{Nonlinear random matrix theory for deep learning}},
year = {2017}
}
@article{Aubin2018,
abstract = {Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.},
archivePrefix = {arXiv},
arxivId = {1806.05451},
author = {Aubin, Benjamin and Maillard, Antoine and Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'{a}}, Lenka},
eprint = {1806.05451},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1806.05451.pdf:pdf},
pages = {1--44},
title = {{The committee machine: Computational to statistical gaps in learning a two-layers neural network}},
url = {http://arxiv.org/abs/1806.05451},
year = {2018}
}
@article{Chung2017,
abstract = {Perceptual manifolds arise when a neural population responds to an ensemble of sensory signals associated with different physical features (e.g., orientation, pose, scale, location, and intensity) of the same perceptual object. Object recognition and discrimination requires classifying the manifolds in a manner that is insensitive to variability within a manifold. How neuronal systems give rise to invariant object classification and recognition is a fundamental problem in brain theory as well as in machine learning. Here we study the ability of a readout network to classify objects from their perceptual manifold representations. We develop a statistical mechanical theory for the linear classification of manifolds with arbitrary geometry revealing a remarkable relation to the mathematics of conic decomposition. Novel geometrical measures of manifold radius and manifold dimension are introduced which can explain the classification capacity for manifolds of various geometries. The general theory is demonstrated on a number of representative manifolds, including L2 ellipsoids prototypical of strictly convex manifolds, L1 balls representing polytopes consisting of finite sample points, and orientation manifolds which arise from neurons tuned to respond to a continuous angle variable, such as object orientation. The effects of label sparsity on the classification capacity of manifolds are elucidated, revealing a scaling relation between label sparsity and manifold radius. Theoretical predictions are corroborated by numerical simulations using recently developed algorithms to compute maximum margin solutions for manifold dichotomies. Our theory and its extensions provide a powerful and rich framework for applying statistical mechanics of linear classification to data arising from neuronal responses to object stimuli, as well as to artificial deep networks trained for object recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1710.06487},
author = {Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
doi = {10.1103/PhysRevX.8.031003},
eprint = {1710.06487},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1710.06487.pdf:pdf},
issn = {2160-3308},
pages = {1--27},
title = {{Classification and Geometry of General Perceptual Manifolds}},
url = {http://arxiv.org/abs/1710.06487{\%}0Ahttp://dx.doi.org/10.1103/PhysRevX.8.031003},
year = {2017}
}
@article{Sokolov2012,
abstract = {A suspension of microswimmers, the simplest realization of active matter, exhibits novel material properties: the emergence of collective motion, reduction in viscosity, increase in diffusivity, and extraction of useful energy. Bacterial dynamics in dilute suspensions suggest that hydrodynamic interactions and collisions between the swimmers lead to collective motion at higher concentrations. On the example of aerobic bacteria Bacillus subtilis, we report on spatial and temporal correlation functions measurements of collective state for various swimming speeds and concentrations. The experiments produced a puzzling result: while the energy injection rate is proportional to the swimming speed and concentration, the correlation length remains practically constant upon small speeds where random tumbling of bacteria dominates. It highlights two fundamental mechanisms: hydrodynamic interactions and collisions; for both of these mechanisms, the change of the swimming speed or concentration alters an overall time scale.},
archivePrefix = {arXiv},
arxivId = {0710.3256v2},
author = {Sokolov, Andrey and Aranson, Igor S.},
doi = {10.1103/PhysRevLett.109.248109},
eprint = {0710.3256v2},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0710.3256.pdf:pdf},
isbn = {0066-4189},
issn = {00319007},
journal = {Physical Review Letters},
number = {24},
pages = {1--58},
pmid = {23368392},
title = {{Physical properties of collective motion in suspensions of bacteria}},
volume = {109},
year = {2012}
}
@article{Montanari2007,
author = {Montanari, Andrea},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/lecture-11.pdf:pdf},
journal = {Stat},
number = {5},
pages = {1--4},
title = {{Bethe-Peierls approximation : an informal introduction Bethe equations Bethe free entropy}},
volume = {1},
year = {2007}
}
@article{Gardner1988,
abstract = {The authors calculate the number, p= alpha N of random N-bit patterns that an optimal neural network can store allowing a given fraction f of bit errors and with the condition that each right bit is stabilised by a local field at least equal to a parameter K. For each value of alpha and K, there is a minimum fraction f min of wrong bits. They find a critical line, alpha c (K) with alpha c (0)=2. The minimum fraction of wrong bits vanishes for alpha {\textless} alpha c (K) and increases from zero for alpha {\textgreater} alpha c (K). The calculations are done using a saddle-point method and the order parameters at the saddle point are assumed to be replica symmetric. This solution is locally stable in a finite region of the K, alpha plane including the line, alpha c (K) but there is a line above which the solution becomes unstable and replica symmetry must be broken.},
author = {Gardner, E and Derrida, B},
doi = {10.1088/0305-4470/21/1/031},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/2. Optimal storage properties of neural network model.pdf:pdf},
issn = {0305-4470},
journal = {Journal of Physics A: Mathematical and General},
number = {1},
pages = {271},
title = {{Optimal storage properties of neural network models}},
url = {http://stacks.iop.org/0305-4470/21/i=1/a=031},
volume = {21},
year = {1988}
}
@article{Diffusion1995a,
author = {Diffusion, Atomic},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/41. Weight space structure and representations.pdf:pdf},
number = {12},
pages = {2364--2367},
title = {{Physical review letters 18}},
volume = {75},
year = {1995}
}
@article{Majer1993,
author = {Majer, P. and Engel, A. and Zippelius, A.},
doi = {10.1088/0305-4470/26/24/015},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/P{\_}Majer{\_}1993{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Gen.{\_}26{\_}015.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {24},
pages = {7405--7416},
title = {{Perceptrons above saturation}},
volume = {26},
year = {1993}
}
@article{Mezard1989,
abstract = {Gardner's computation of the number of N-bit patterns which can be stored in an optimal neural network used as an associative memory is derived without replicas, using the cavity method. This allows for a unified presentation whatever the basic measure in the space of coupling constants, but above all it gives the clear physical content of the assumption of replica symmetry. TAP equations are also derived. Foreword One of the most exciting recent developments in the theory of neural networks is a contribution of Elizabeth Gardner. She showed how one can analyse the space of all the networks that are able to memorise a certain number of patterns, using spin-glass techniques and ideas. This was a major step forward and this kind of computation 'B la Gardner' (as it is often referred to among specialists) has been one of the most useful tools in the theory of neural networks since. As a tribute to my greatly missed colleague Elizabeth, to her talent and modesty, I decided to write down the following notes, which present an alternative and complementary derivation of her results.},
author = {M{\'{e}}zard, Marc},
doi = {10.1088/0305-4470/22/12/018},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/89{\_}M{\_}JPA.pdf:pdf},
isbn = {0305-4470},
issn = {13616447},
journal = {Journal of Physics A: Mathematical and General},
number = {12},
pages = {2181--2190},
title = {{The space of interactions in neural networks: Gardner'scomputation with the cavity method}},
volume = {22},
year = {1989}
}
@article{Huang2013,
abstract = {The statistical picture of the solution space for a binary perceptron is studied. The binary perceptron learns a random classification of input random patterns by a set of binary synaptic weights. The learning of this network is difficult especially when the pattern (constraint) density is close to the capacity, which is supposed to be intimately related to the structure of the solution space. The geometrical organization is elucidated by the entropy landscape from a reference configuration and of solution-pairs separated by a given Hamming distance in the solution space. We evaluate the entropy at the annealed level as well as replica symmetric level and the mean field result is confirmed by the numerical simulations on single instances using the proposed message passing algorithms. From the first landscape (a random configuration as a reference), we see clearly how the solution space shrinks as more constraints are added. From the second landscape of solution-pairs, we deduce the coexistence of clustering and freezing in the solution space.},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.2850v2},
author = {Huang, Haiping and Wong, K. Y.Michael and Kabashima, Yoshiyuki},
doi = {10.1088/1751-8113/46/37/375002},
eprint = {arXiv:1304.2850v2},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1304.2850.pdf:pdf},
issn = {17518113},
journal = {Journal of Physics A: Mathematical and Theoretical},
number = {37},
title = {{Entropy landscape of solutions in the binary perceptron problem}},
volume = {46},
year = {2013}
}
@article{Thouless1977,
author = {Thouless, D J and Anderson, P W and Palmer, R G},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/2. Solution of Solvable model of a spin glass $\backslash$: Thouless - Anderson - Palmer.pdf:pdf},
journal = {Phil. Mag.},
number = {3},
pages = {593--601},
title = {{Solution of `Solvable Model of a Spin-Glass'}},
volume = {35},
year = {1977}
}
@article{Wilhelm2010,
abstract = {In this article we present tmvtnorm, an R package implementation for the truncated multivariate normal distribution. We consider random number generation with rejection and Gibbs sampling, computation of marginal densi-ties as well as computation of the mean and co-variance of the truncated variables. This contri-bution brings together latest research in this field and provides useful methods for both scholars and practitioners when working with truncated normal variables.},
author = {Wilhelm, Stefan and BG, BGM},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/RJournal{\_}2010-1{\_}Wilhelm+Manjunath.pdf:pdf},
issn = {20734859},
journal = {Sigma},
number = {June},
pages = {25--29},
title = {{tmvtnorm: A package for the truncated multivariate normal distribution}},
url = {http://journal.r-project.org/archive/2010-1/RJournal{\_}2010-1{\_}Wilhelm+Manjunath{~}B{~}G.pdf},
volume = {2},
year = {2010}
}
@article{Krauth1989a,
author = {Krauth, W and Mezard, Marc},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/5. Storage capacity of memory networks with binary couplings.pdf:pdf},
journal = {J. Phys (France)},
pages = {3057--3066},
title = {{Storage capacity of memory networks with binary coupling}},
volume = {50},
year = {1989}
}
@article{Mei2018,
abstract = {Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties? In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearlyideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.},
archivePrefix = {arXiv},
arxivId = {1804.06561},
author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
eprint = {1804.06561},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1804.06561.pdf:pdf},
title = {{A Mean Field View of the Landscape of Two-Layers Neural Networks}},
url = {http://arxiv.org/abs/1804.06561},
year = {2018}
}
@article{Huang2014a,
abstract = {Supervised learning in a binary perceptron is able to classify an extensive number of random patterns by a proper assignment of binary synaptic weights. However, to find such assignments in practice, is quite a nontrivial task. The relation between the weight space structure and the algorithmic hardness has not yet been fully understood. To this end, we analytically derive the Franz-Parisi potential for the binary preceptron problem, by starting from an equilibrium solution of weights and exploring the weight space structure around it. Our result reveals the geometrical organization of the weight space$\backslash$textemdash the weight space is composed of isolated solutions, rather than clusters of exponentially many close-by solutions. The point-like clusters far apart from each other in the weight space explain the previously observed glassy behavior of stochastic local search heuristics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1784v1},
author = {Huang, Haiping and Kabashima, Yoshiyuki},
doi = {10.1103/PhysRevE.90.052813},
eprint = {arXiv:1408.1784v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/35. Origin of the computational hardness for learning.pdf:pdf},
issn = {15502376},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {5},
title = {{Origin of the computational hardness for learning with binary synapses}},
volume = {90},
year = {2014}
}
@article{Georges1991,
abstract = {Abstraet. High - temperature expansions performed at a fired-order parameter provide a simple and systematic way to derive and correct mean - field theories for statistical mechanical models. For models like spin glasses which have general couplings between ... $\backslash$n},
author = {Georges, Antoine and Yedidia, Jonathan S},
doi = {10.1088/0305-4470/24/9/024},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/A{\_}Georges{\_}1991{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Gen.{\_}24{\_}024.pdf:pdf},
isbn = {0305-4470},
issn = {0305-4470},
journal = {Journal of Physics A: Mathematical and General},
pages = {2173--2192},
title = {{How to expand around mean-field theory using high-temperature expansions}},
volume = {24},
year = {1991}
}
@article{BenArous2017,
abstract = {We consider the problem of estimating a large rank-one tensor {\$}{\{}\backslashboldsymbol u{\}}{\^{}}{\{}\backslashotimes k{\}}\backslashin({\{}\backslashmathbb R{\}}{\^{}}{\{}n{\}}){\^{}}{\{}\backslashotimes k{\}}{\$}, {\$}k\backslashge 3{\$} in Gaussian noise. Earlier work characterized a critical signal-to-noise ratio {\$}\backslashlambda{\_}{\{}Bayes{\}}= O(1){\$} above which an ideal estimator achieves strictly positive correlation with the unknown vector of interest. Remarkably no polynomial-time algorithm is known that achieved this goal unless {\$}\backslashlambda\backslashge C n{\^{}}{\{}(k-2)/4{\}}{\$} and even powerful semidefinite programming relaxations appear to fail for {\$}1\backslashll \backslashlambda\backslashll n{\^{}}{\{}(k-2)/4{\}}{\$}. In order to elucidate this behavior, we consider the maximum likelihood estimator, which requires maximizing a degree-{\$}k{\$} homogeneous polynomial over the unit sphere in {\$}n{\$} dimensions. We compute the expected number of critical points and local maxima of this objective function and show that it is exponential in the dimensions {\$}n{\$}, and give exact formulas for the exponential growth rate. We show that (for {\$}\backslashlambda{\$} larger than a constant) critical points are either very close to the unknown vector {\$}{\{}\backslashboldsymbol u{\}}{\$}, or are confined in a band of width {\$}\backslashTheta(\backslashlambda{\^{}}{\{}-1/(k-1){\}}){\$} around the maximum circle that is orthogonal to {\$}{\{}\backslashboldsymbol u{\}}{\$}. For local maxima, this band shrinks to be of size {\$}\backslashTheta(\backslashlambda{\^{}}{\{}-1/(k-2){\}}){\$}. These `uninformative' local maxima are likely to cause the failure of optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1711.05424},
author = {{Ben Arous}, Gerard and Mei, Song and Montanari, Andrea and Nica, Mihai},
eprint = {1711.05424},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1711.05424.pdf:pdf},
pages = {1--40},
title = {{The landscape of the spiked tensor model}},
url = {http://arxiv.org/abs/1711.05424},
year = {2017}
}
@article{Schniter2016,
abstract = {The generalized linear model (GLM), where a random vector {\$}\backslashboldsymbol{\{}x{\}}{\$} is observed through a noisy, possibly nonlinear, function of a linear transform output {\$}\backslashboldsymbol{\{}z{\}}=\backslashboldsymbol{\{}Ax{\}}{\$}, arises in a range of applications such as robust regression, binary classification, quantized compressed sensing, phase retrieval, photon-limited imaging, and inference from neural spike trains. When {\$}\backslashboldsymbol{\{}A{\}}{\$} is large and i.i.d. Gaussian, the generalized approximate message passing (GAMP) algorithm is an efficient means of MAP or marginal inference, and its performance can be rigorously characterized by a scalar state evolution. For general {\$}\backslashboldsymbol{\{}A{\}}{\$}, though, GAMP can misbehave. Damping and sequential-updating help to robustify GAMP, but their effects are limited. Recently, a "vector AMP" (VAMP) algorithm was proposed for additive white Gaussian noise channels. VAMP extends AMP's guarantees from i.i.d. Gaussian {\$}\backslashboldsymbol{\{}A{\}}{\$} to the larger class of rotationally invariant {\$}\backslashboldsymbol{\{}A{\}}{\$}. In this paper, we show how VAMP can be extended to the GLM. Numerical experiments show that the proposed GLM-VAMP is much more robust to ill-conditioning in {\$}\backslashboldsymbol{\{}A{\}}{\$} than damped GAMP.},
archivePrefix = {arXiv},
arxivId = {1612.01186},
author = {Schniter, Philip and Rangan, Sundeep and Fletcher, Alyson K.},
eprint = {1612.01186},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1612.01186.pdf:pdf},
title = {{Vector Approximate Message Passing for the Generalized Linear Model}},
url = {http://arxiv.org/abs/1612.01186},
year = {2016}
}
@article{Gualdi2015,
abstract = {We propose a simple framework to understand commonly observed crisis waves in macroeconomic Agent Based models, that is also relevant to a variety of other physical or biological situations where synchronization occurs. We compute exactly the phase diagram of the model and the location of the synchronization transition in parameter space. Many modifications and extensions can be studied, confirming that the synchronization transition is extremely robust against various sources of noise or imperfections. },
archivePrefix = {arXiv},
arxivId = {1409.3296},
author = {Gualdi, Stanislao and Bouchaud, Jean Philippe and Cencetti, Giulia and Tarzia, Marco and Zamponi, Francesco},
doi = {10.1103/PhysRevLett.114.088701},
eprint = {1409.3296},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1409.3296v1.pdf:pdf},
issn = {10797114},
journal = {Physical Review Letters},
number = {8},
pages = {1--5},
title = {{Endogenous crisis waves: Stochastic model with synchronized collective Behavior}},
volume = {114},
year = {2015}
}
@article{Huang2013a,
abstract = {The statistical picture of the solution space for a binary perceptron is studied. The binary perceptron learns a random classification of input random patterns by a set of binary synaptic weights. The learning of this network is difficult especially when the pattern (constraint) density is close to the capacity, which is supposed to be intimately related to the structure of the solution space. The geometrical organization is elucidated by the entropy landscape from a reference configuration and of solution-pairs separated by a given Hamming distance in the solution space. We evaluate the entropy at the annealed level as well as replica symmetric level and the mean field result is confirmed by the numerical simulations on single instances using the proposed message passing algorithms. From the first landscape (a random configuration as a reference), we see clearly how the solution space shrinks as more constraints are added. From the second landscape of solution-pairs, we deduce the coexistence of clustering and freezing in the solution space.},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.2850v2},
author = {Huang, Haiping and Wong, K. Y.Michael and Kabashima, Yoshiyuki},
doi = {10.1088/1751-8113/46/37/375002},
eprint = {arXiv:1304.2850v2},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/34. Entropy landscape of solutions in the binary perceptron problem.pdf:pdf},
issn = {17518113},
journal = {Journal of Physics A: Mathematical and Theoretical},
number = {37},
title = {{Entropy landscape of solutions in the binary perceptron problem}},
volume = {46},
year = {2013}
}
@article{Chaudhari2016,
abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
archivePrefix = {arXiv},
arxivId = {1611.01838},
author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
eprint = {1611.01838},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1611.01838.pdf:pdf},
isbn = {978-3-642-04273-7},
pages = {1--19},
title = {{Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}},
url = {http://arxiv.org/abs/1611.01838},
year = {2016}
}
@misc{Sherrington1975,
author = {Sherrington, D and Kirkpatrick, S},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/3. Solvable model of a spin glass - D.Sherrington S.Kirkpatrick.pdf:pdf},
pages = {1792--1796},
title = {{Solvable Model of a Spin Glass}},
volume = {35},
year = {1975}
}
@article{Obuchi2009,
abstract = {The weight space of the Ising perceptron in which a set of random patterns is stored is examined using the generating function of the partition function {\$}\backslashphi(n)=(1/N)\backslashlog [Z{\^{}}n]{\$} as the dimension of the weight vector {\$}N{\$} tends to infinity, where {\$}Z{\$} is the partition function and {\$}[ ... ]{\$} represents the configurational average. We utilize {\$}\backslashphi(n){\$} for two purposes, depending on the value of the ratio {\$}\backslashalpha=M/N{\$}, where {\$}M{\$} is the number of random patterns. For {\$}\backslashalpha {\textless} \backslashalpha{\_}{\{}\backslashrm s{\}}=0.833 ...{\$}, we employ {\$}\backslashphi(n){\$}, in conjunction with Parisi's one-step replica symmetry breaking scheme in the limit of {\$}n \backslashto 0{\$}, to evaluate the complexity that characterizes the number of disjoint clusters of weights that are compatible with a given set of random patterns, which indicates that, in typical cases, the weight space is equally dominated by a single large cluster of exponentially many weights and exponentially many small clusters of a single weight. For {\$}\backslashalpha {\textgreater} \backslashalpha{\_}{\{}\backslashrm s{\}}{\$}, on the other hand, {\$}\backslashphi(n){\$} is used to assess the rate function of a small probability that a given set of random patterns is atypically separable by the Ising perceptrons. We show that the analyticity of the rate function changes at {\$}\backslashalpha = \backslashalpha{\_}{\{}\backslashrm GD{\}}=1.245 ... {\$}, which implies that the dominant configuration of the atypically separable patterns exhibits a phase transition at this critical ratio. Extensive numerical experiments are conducted to support the theoretical predictions.},
archivePrefix = {arXiv},
arxivId = {0910.2281},
author = {Obuchi, Tomoyuki and Kabashima, Yoshiyuki},
doi = {10.1088/1742-5468/2009/12/P12014},
eprint = {0910.2281},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Obuchi{\_}2009{\_}J.{\_}Stat.{\_}Mech.{\_}2009{\_}P12014(2).pdf:pdf},
issn = {1742-5468},
title = {{Weight space structure and analysis using a finite replica number in the Ising perceptron}},
url = {http://arxiv.org/abs/0910.2281{\%}0Ahttp://dx.doi.org/10.1088/1742-5468/2009/12/P12014},
year = {2009}
}
@article{Seung1992,
abstract = {Learning from examples in feedforward neural networks is studied within$\backslash$na statistical-mechanical framework. Training is assumed to be stochastic,$\backslash$nleading to a Gibbs distribution of networks characterized by a temperature$\backslash$nparameter T. Learning of realizable rules as well as of unrealizable$\backslash$nrules is considered. In the latter case, the target rule cannot be$\backslash$nperfectly realized by a network of the given architecture. Two useful$\backslash$napproximate theories of learning from examples are studied: the high-temperature$\backslash$nlimit and the annealed approximation. Exact treatment of the quenched$\backslash$ndisorder generated by the random sampling of the examples leads to$\backslash$nthe use of the replica theory. Of primary interest is the generalization$\backslash$ncurve, namely, the average generalization error ?g versus the number$\backslash$nof examples P used for training. The theory implies that, for a reduction$\backslash$nin ?g that remains finite in the large-N limit, P should generally$\backslash$nscale as ?N, where N is the number of independently adjustable weights$\backslash$nin the network. We show that for smooth networks, i.e., those with$\backslash$ncontinuously varying weights and smooth transfer functions, the generalization$\backslash$ncurve asymptotically obeys an inverse power law. In contrast, for$\backslash$nnonsmooth networks other behaviors can appear, depending on the nature$\backslash$nof the nonlinearities as well as the realizability of the rule. In$\backslash$nparticular, a discontinuous learning transition from a state of poor$\backslash$nto a state of perfect generalization can occur in nonsmooth networks$\backslash$nlearning realizable rules. We illustrate both gradual and continuous$\backslash$nlearning with a detailed analytical and numerical study of several$\backslash$nsingle-layer perceptron models. Comparing with the exact replica$\backslash$ntheory of perceptron learning, we find that for realizable rules$\backslash$nthe high-temperature and annealed theories provide very good approximations$\backslash$nto the generalization performance. Assuming this to hold for multilayer$\backslash$nnetworks as well, we propose a classification of possible asymptotic$\backslash$nforms of learning curves in general realizable models. For unrealizable$\backslash$nrules we find that the above approximations fail in general to predict$\backslash$ncorrectly the shapes of the generalization curves. Another indication$\backslash$nof the important role of quenched disorder for unrealizable rules$\backslash$nis that the generalization error is not necessarily a monotonically$\backslash$nincreasing function of temperature. Also, unrealizable rules can$\backslash$npossess genuine spin-glass phases indicative of degenerate minima$\backslash$nseparated by high barriers.},
author = {Seung, Hs and Sompolinsky, H and Tishby, N},
doi = {10.1103/PhysRevA.45.6056},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/38. Statistical mechanics of learning from examples.pdf:pdf},
isbn = {1050-2947 (Print)$\backslash$r1050-2947 (Linking)},
issn = {1050-2947},
journal = {Physical Review A},
number = {8},
pages = {6056--6091},
pmid = {9907706},
title = {{Statistical mechanics of learning from examples}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9907706{\%}5Cnhttp://link.aps.org/doi/10.1103/PhysRevA.45.6056},
volume = {45},
year = {1992}
}
@article{Barkai1990,
abstract = {Statistical mechanics is applied to estimate the maximal information$\backslash$ncapacity per synapse (?c) of a multilayered feedforward neural network,$\backslash$nfunctioning as a parity machine. For a large number of hidden units,$\backslash$nK, the replica-symmetric solution overestimates dramatically the$\backslash$ncapacity, ?c?K2. However, a one-step replica-symmetry breaking gives$\backslash$n?c?lnK/ln2, which coincides with a theoretical upper bound. It is$\backslash$nsuggested that this asymptotic behavior is exact. Results for finite$\backslash$nK are also discussed.},
author = {Barkai, E. and Hansel, D. and Kanter, I.},
doi = {10.1103/PhysRevLett.65.2312},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/40. Statistical mechanics of feedforward neural networks.pdf:pdf},
issn = {00319007},
journal = {Physical Review Letters},
number = {18},
pages = {2312--2315},
pmid = {10042513},
title = {{Statistical mechanics of a multilayered neural network}},
volume = {65},
year = {1990}
}
@article{Sakata2012,
abstract = {Finding a basis matrix (dictionary) by which objective signals are represented sparsely is of major relevance in various scientific and technological fields. We consider a problem to learn a dictionary from a set of training signals. We employ techniques of statistical mechanics of disordered systems to evaluate the size of the training set necessary to typically succeed in the dictionary learning. The results indicate that the necessary size is much smaller than previously estimated, which theoretically supports and/or encourages the use of dictionary learning in practical situations.},
archivePrefix = {arXiv},
arxivId = {1203.6178v2},
author = {Sakata, Ayaka and Kabashima, Yoshiyuki},
doi = {10.1209/0295-5075/103/28008},
eprint = {1203.6178v2},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Sakata{\_}2013{\_}EPL{\_}103{\_}28008.pdf:pdf},
issn = {02955075},
journal = {Europhysics Letters},
pages = {2--5},
title = {{Statistical Mechanics of Dictionary Learning}},
url = {http://arxiv.org/abs/1203.6178},
volume = {28008},
year = {2012}
}
@article{Watkin1993,
abstract = {A summary is presented of the statistical mechanical theory of learning$\backslash$na rule with a neural network, a rapidly advancing area which is$\backslash$nclosely related to other inverse problems frequently encountered$\backslash$nby physicists. By emphasizing the relationship between neural networks$\backslash$nand strongly interacting physical systems, such as spin glasses,$\backslash$nthe authors show how learning theory has provided a workshop in$\backslash$nwhich to develop new, exact analytical techniques.},
author = {Watkin, Timothy L H and Rau, Albrecht},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/49. The statistical mechanics of learning a rule.pdf:pdf},
journal = {Review of Modern Physics},
keywords = {reviews},
pages = {499--556},
title = {{The statistical mechanics of learning a rule}},
volume = {65},
year = {1993}
}
@article{Mezard2011,
abstract = {We discuss the analytical solution through the cavity method of a mean field model that displays at the same time an ideal glass transition and a set of jamming points. We establish the equations describing this system, and we discuss some approximate analytical solutions and a numerical strategy to solve them exactly. We compare these methods and we get insight into the reliability of the theory for the description of finite dimensional hard spheres.},
archivePrefix = {arXiv},
arxivId = {1011.5080},
author = {M{\'{e}}zard, Marc and Parisi, Giorgio and Tarzia, Marco and Zamponi, Francesco},
doi = {10.1088/1742-5468/2011/03/P03002},
eprint = {1011.5080},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Mezard-Parisi-Tarzia-Zamponi-On the solution of a 'solvable' model of an ideal glass of hard spheres displaying a jamming transition .pdf:pdf},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {cavity and replica method,classical phase transitions (theory),phase diagrams (theory),structural glasses (theory)},
number = {3},
title = {{On the solution of a 'solvable' model of an ideal glass of hard spheres displaying a jamming transition}},
volume = {2011},
year = {2011}
}
@article{Pennington2017,
abstract = {Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix Y T Y , Y = f (W X), where W is a random weight matrix, X is a random data matrix, and f is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature networks on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties.},
author = {Pennington, Jeffrey and Brain, Google and Worah, Pratik},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf:pdf},
journal = {Nips},
number = {31},
title = {{Nonlinear random matrix theory for deep learning}},
year = {2017}
}
@article{Saad1995a,
abstract = {We present an analytic solution to the problem of on-line gradient-descent learning for two-layer neural networks with an arbitrary number of hidden units in both teacher and student networks. PACS numbers: 87. 10.+e, 02.50. — r, 05.20. — y Layered neural networks are of interest for their abil-ity to implement input-output maps [1]. Classification and regression tasks formulated as a map from an N-dimensional input space g onto a scalar g are real-ized through a map g = fi(g), which can be modified through changes in the internal parameters [J) specifying the strength of the interneuron couplings [2,3]. Learn-ing refers to the modification of these couplings so as to bring the map f{\~{}} implemented by the network as close as possible to a desired map f. The degree of success is monitored through the generalization error, a measure of the dissimilarity between f{\~{}} and f. Learning from examples in layered neural networks is usually formulated as an optimization problem [2,3], based on the minimization of an additive learning er-ror defined over a training set composed of P inde-pendent examples ((t', ("), with (t' = f(g"), 1 {\~{}} p {\~{}} P. Statistical physics tools for investigating the prop-erties of such models, based on the use of the replica method, have been successfully applied to the analysis of single-layer perceptrons [3] and some simplified two-layer structures (e.g., committee machines [4]). Analysis of more complicated multilayer networks is hampered by technical difficulties due to the complex structure of the solutions in a space of order parameters [5], which de-scribe in this case correlations among the various neurons in the trained network, as well as their degree of special-ization toward the implementation of the desired task. A recently introduced alternative approach investigates on line learning -[6]. In this scenario the couplings are adjusted to minimize the error after the presentation of each example. The resulting changes in [J] are de-scribed as a dynamical evolution, with the number of examples playing the role of time. The average that ac-counts for the disorder introduced by the independent random selection of an example at each time step can be performed directly, without invoking the replica method. The resulting equations of motion for the relevant order parameters characterize the structure of the space of so-lutions and allow for a computation of the generalization error. While investigating the on-line learning scenario pro-posed by Biehl and Schwarze [6], we found an unexpected result: The dynamical equations for the order parameters can be obtained analytically for a general two-layer stu-dent network composed of N input units, K hidden units, and a single linear output unit, trained to perform a task defined through a teacher network of similar architecture, except that its number M of hidden units is not necessarily equal to K. Two-layer networks with an arbitrary number of hidden units have been shown to be universal approximators [1] for N-to-one dimensional maps. Our results thus describe the learning of tasks of arbitrary complexity (general M). The complexity of the student network is also arbitrary (general K, independent of M), providing a tool to investigate realizable (K = M), overrealizable (K) M), and unrealizable (K (M) learning scenarios. Such capabilities are to be contrasted with previously available results; the equations provided in [6] can only describe a committee rnachine with K = 2 hidden units learning a linearly separable task (M = 1). In this Letter we limit our discussion to the case of the soft-committee machine [6], in which all the hid-den units are connected to the output unit with positive couplings of unit strength, and only the input-to-hidden couplings are adaptive. Consider the student network: hid-den unit i receives information from input unit r through the weight J; " and its activation under presentation of an input pattern g = (gi, . . . , g{\~{}}) is x; = J; g, with J; = (J;i, . . . , J,{\~{}}) defined as the vector of incoming weights onto the ith hidden unit. The output of the student network is tr(J, g) = g, , g(J; g), where g is the activation func-tion of the hidden units, taken here to be the error function g(x) = — erf (x/{\~{}}2), and J — = [J;)i; tc is the set of input-to-hidden adaptive weights. Training examples are of the form (gt', gt'). The components of the independently drawn input vectors g{\&} are uncorrelated random variables with zero mean and unit variance. The corresponding output gP is given by a deterministic teacher whose internal structure is that of a network similar to the student except for a possible difference in the number M of hidden units. Hidden unit n in the teacher network receives input information through the weight vector B, = (B " i, . . . , B " tv), and its activation under presentation of the input pattern g{\&} is yn The corresponding output is g{\&} = g " ,g(B, gt").},
author = {Saad, David and Solla, Sara A.},
doi = {10.1103/PhysRevLett.74.4337},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.74.4337.pdf:pdf},
issn = {00319007},
journal = {Physical Review Letters},
number = {21},
pages = {4337--4340},
title = {{Exact solution for on-line learning in multilayer neural networks}},
volume = {74},
year = {1995}
}
@article{Montanari2018,
author = {Montanari, Andrea},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Montanari.pdf:pdf},
pages = {1--4},
title = {{Lecture on neural networks}},
volume = {2},
year = {2018}
}
@article{Park1996,
author = {Park, Kibeom and Kwon, C. and Park, Youngah},
doi = {10.1088/0305-4470/29/7/012},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Kibeom{\_}Park{\_}1996{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Gen.{\_}29{\_}012.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {7},
pages = {1397--1409},
title = {{One-step replica-symmetry-breaking solution for a perceptron learning with weight mismatch}},
volume = {29},
year = {1996}
}
@article{Crisanti2011,
abstract = {To test the stability of the Parisi solution near T=0, we study the spectrum of the Hessian of the Sherrington-Kirkpatrick model near T=0, whose eigenvalues are the masses of the bare propagators in the expansion around the mean-field solution. In the limit {\$}T\backslashll 1{\$} two regions can be identified. In the first region, for {\$}x{\$} close to 0, where {\$}x{\$} is the Parisi replica symmetry breaking scheme parameter, the spectrum of the Hessian is not trivial and maintains the structure of the full replica symmetry breaking state found at higher temperatures. In the second region {\$}T\backslashll x \backslashleq 1{\$} as {\$}T\backslashto 0{\$}, the components of the Hessian become insensitive to changes of the overlaps and the bands typical of the full replica symmetry breaking state collapse. In this region only two eigenvalues are found: a null one and a positive one, ensuring stability for {\$}T\backslashll 1{\$}. In the limit {\$}T\backslashto 0{\$} the width of the first region shrinks to zero and only the positive and null eigenvalues survive. As byproduct we enlighten the close analogy between the static Parisi replica symmetry breaking scheme and the multiple time-scales approach of dynamics, and compute the static susceptibility showing that it equals the static limit of the dynamic susceptibility computed via the modified fluctuation dissipation theorem.},
archivePrefix = {arXiv},
arxivId = {1101.5233},
author = {Crisanti, A and {De Dominicis}, C},
doi = {10.1088/1751-8113/44/11/115006},
eprint = {1101.5233},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1101.5233.pdf:pdf},
issn = {1751-8113},
journal = {Journal of Physics A: Mathematical and Theoretical},
number = {11},
pages = {115006},
title = {{Stability of the Parisi solution for the Sherrington–Kirkpatrick model near {\textless}i{\textgreater}T{\textless}/i{\textgreater} = 0}},
url = {http://stacks.iop.org/1751-8121/44/i=11/a=115006?key=crossref.cd8e9b19bfc5371977c75151104002a5},
volume = {44},
year = {2011}
}
@article{Krzakala2012,
abstract = {Compressed sensing is a signal processing method that acquires data directly in a compressed form. This allows one to make less measurements than what was considered necessary to record a signal, enabling faster or more precise measurement protocols in a wide range of applications. Using an interdisciplinary approach, we have recently proposed in [arXiv:1109.4424] a strategy that allows compressed sensing to be performed at acquisition rates approaching to the theoretical optimal limits. In this paper, we give a more thorough presentation of our approach, and introduce many new results. We present the probabilistic approach to reconstruction and discuss its optimality and robustness. We detail the derivation of the message passing algorithm for reconstruction and expectation max- imization learning of signal-model parameters. We further develop the asymptotic analysis of the corresponding phase diagrams with and without measurement noise, for different distribution of signals, and discuss the best possible reconstruction performances regardless of the algorithm. We also present new efficient seeding matrices, test them on synthetic data and analyze their performance asymptotically.},
archivePrefix = {arXiv},
arxivId = {1206.3953},
author = {Krzakala, Florent and M{\'{e}}zard, Marc and Sausset, Francois and Sun, Yifan and Zdeborov{\'{a}}, Lenka},
doi = {10.1088/1742-5468/2012/08/P08009},
eprint = {1206.3953},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/31. Probabilistic Reconstruction in Compressed Sensing Algorithms Phase Diagrams and Threshold Achieving Matrices.pdf:pdf},
isbn = {1742-5468},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {cavity and replica method,error correcting codes,message-passing algorithms,statistical inference},
number = {8},
pages = {1--42},
title = {{Probabilistic reconstruction in compressed sensing: Algorithms, phase diagrams, and threshold achieving matrices}},
volume = {2012},
year = {2012}
}
@article{Interno2015,
author = {Interno, Relatore and {Caracciolo Correlatore}, Sergio and {Zecchina Correlatore}, Riccardo and {Carlo Lucibello}, Dott},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Amelio.pdf:pdf},
title = {{Corso Di Laurea Magistrale in Fisica Out-of-Equilibrium Analysis of Simple Neural Networks}},
year = {2015}
}
@article{Feizi2017,
abstract = {Neural networks have been used prominently in several machine learning and statistics applications. In general, the underlying optimization of neural networks is non-convex which makes their performance analysis challenging. In this paper, we take a novel approach to this problem by asking whether one can constrain neural network weights to make its optimization landscape have good theoretical properties while at the same time, be a good approximation for the unconstrained one. For two-layer neural networks, we provide affirmative answers to these questions by introducing Porcupine Neural Networks (PNNs) whose weight vectors are constrained to lie over a finite set of lines. We show that most local optima of PNN optimizations are global while we have a characterization of regions where bad local optimizers may exist. Moreover, our theoretical and empirical results suggest that an unconstrained neural network can be approximated using a polynomially-large PNN.},
archivePrefix = {arXiv},
arxivId = {1710.02196},
author = {Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
eprint = {1710.02196},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1710.02196.pdf:pdf},
number = {Figure 1},
title = {{Porcupine Neural Networks: (Almost) All Local Optima are Global}},
url = {http://arxiv.org/abs/1710.02196},
year = {2017}
}
@article{Hertz1993,
abstract = {We study generalization in large committee machines. For a model with nonoverlapping receptive fields a full replica calculation yields results qualitatively similar to those for single-layer machines. For a fully connected architecture, within the annealed approximation we find a transition from a symmetric state to one with specialized hidden units, accompanied by a discontinuous drop in the generalization error, for both binary and continuous weights. The poorly generalizing symmetric states are metastable for arbitrarily large training sets. {\textcopyright} 1993.},
author = {Hertz, John and Schwarze, Holm},
doi = {10.1016/0378-4371(93)90561-H},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/55.Dicontinuous Generalization in large comiittee machines.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
number = {1-4},
pages = {563--569},
title = {{Generalization in large committee machines}},
volume = {200},
year = {1993}
}
@article{Gueudre2014,
abstract = {Finding a good compromise between the exploitation of known resources and the exploration of unknown, but potentially more profitable choices, is a general problem, which arises in many different scientific disciplines. We propose a stylized model for these exploration-exploitation situations, including population or economic growth, portfolio optimisation, evolutionary dynamics, or the problem of optimal pinning of vortices or dislocations in disordered materials. We find the exact growth rate of this model for tree-like geometries and prove the existence of an optimal migration rate in this case. Numerical simulations in the one-dimensional case confirm the generic existence of an optimum.},
archivePrefix = {arXiv},
arxivId = {1310.5114},
author = {Gueudr{\'{e}}, Thomas and Dobrinevski, Alexander and Bouchaud, Jean Philippe},
doi = {10.1103/PhysRevLett.112.050602},
eprint = {1310.5114},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1310.5114.pdf:pdf},
issn = {00319007},
journal = {Physical Review Letters},
number = {5},
pmid = {24580581},
title = {{Explore or exploit? A generic model and an exactly solvable case}},
volume = {112},
year = {2014}
}
@article{Plefka2002,
abstract = {The stability of the TAP mean-field equations is reanalyzed with the conclusion that the exclusive reason for the breakdown at the spin glass instability is an inconsistency for the value of the local susceptibility. A new alternative approach leads to modified equations which are in complete agreement with the original ones above the instability. Essentially altered results below the instability are presented and the consequences for the dynamical mean-field equations are discussed.},
author = {Plefka, T.},
doi = {10.1209/epl/i2002-00457-7},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/T.{\_}Plefka{\_}2002{\_}EPL{\_}58{\_}892.pdf:pdf},
issn = {02955075},
journal = {Europhysics Letters},
number = {6},
pages = {892--898},
title = {{Modified TAP equations for the SK spin glass}},
volume = {58},
year = {2002}
}
@article{Bouchaud2013,
abstract = {Financial and economic history is strewn with bubbles and crashes, booms and busts, crises and upheavals of all sorts. Understanding the origin of these events is arguably one of the most important problems in economic theory. In this paper, we review recent efforts to include heterogeneities and interactions in models of decision. We argue that the Random Field Ising model (RFIM) indeed provides a unifying framework to account for many collective socio-economic phenomena that lead to sudden ruptures and crises. We discuss different models that can capture potentially destabilising self-referential feedback loops, induced either by herding, i.e. reference to peers, or trending, i.e. reference to the past, and account for some of the phenomenology missing in the standard models. We discuss some empirically testable predictions of these models, for example robust signatures of RFIM-like herding effects, or the logarithmic decay of spatial correlations of voting patterns. One of the most striking result, inspired by statistical physics methods, is that Adam Smith's invisible hand can badly fail at solving simple coordination problems. We also insist on the issue of time-scales, that can be extremely long in some cases, and prevent socially optimal equilibria to be reached. As a theoretical challenge, the study of so-called "detailed-balance" violating decision rules is needed to decide whether conclusions based on current models (that all assume detailed-balance) are indeed robust and generic.},
archivePrefix = {arXiv},
arxivId = {1209.0453},
author = {Bouchaud, Jean Philippe},
doi = {10.1007/s10955-012-0687-3},
eprint = {1209.0453},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/JStatPhys.pdf:pdf},
isbn = {0022-4715},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Avalanches,Collective phenomena,Crises,Random field Ising model},
number = {3-4},
pages = {567--606},
title = {{Crises and Collective Socio-Economic Phenomena: Simple Models and Challenges}},
volume = {151},
year = {2013}
}
@article{Brunel1992,
abstract = {The authors study the information storage capacity of a simple perceptron in the error regime. For random unbiased patterns the geometrical analysis gives a logarithmic dependence for the information content in the asymptotic limit. In this case, the statistical physics approach, when used at the simplest level of replica theory, does not give satisfactory results. However for perceptrons with finite stability, the information content can be simply calculated with statistical physics methods in a region above the critical storage level, for biased as well as for unbiased patterns.},
author = {Brunel, N. and Nadal, J. P. and Toulouse, G.},
doi = {10.1088/0305-4470/25/19/015},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/brunel92.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {19},
pages = {5017--5038},
title = {{Information capacity of a perceptron}},
volume = {25},
year = {1992}
}
@article{Mezard1986a,
abstract = {(Re{\c{c}}u le 27 janvier, accepti le 23 avril 1986) R{\'{e}}sum{\'{e}}. 2014 Nous proposons et analysons une solution sym{\'{e}}trique dans les r{\'{e}}pliques des probl{\`{e}}mes de voyageurs de commerce {\`{a}} liens ind{\'{e}}pendants. Elle fournit des estimations analytiques raisonnables pour les quantit{\'{e}}s thermo-dynamiques comme la longueur du chemin le plus court. Abstract. 2014 We propose and analyse a replica symmetric solution for random link travelling salesman problems. This gives reasonable analytical estimates for thermodynamic quantities such as the length of the shortest path.},
author = {M{\'{e}}zard, M and Parisi, G},
doi = {10.1051/jphys:019860047080128500},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/86{\_}MP{\_}JDP.pdf:pdf},
isbn = {0198600470801},
issn = {0302-0738},
journal = {J. Physique Classification Physics Abstracts},
number = {1986},
pages = {1285--1296},
title = {{A replica analysis of the travelling salesman problem}},
volume = {47},
year = {1986}
}
@article{Fyodorov2018,
abstract = {An encryption of a signal {\$}{\{}\backslashbf s{\}}\backslashin\backslashmathbb{\{}R{\^{}}N{\}}{\$} is a random mapping {\$}{\{}\backslashbf s{\}}\backslashmapsto \backslashtextbf{\{}y{\}}=(y{\_}1,\backslashldots,y{\_}M){\^{}}T\backslashin \backslashmathbb{\{}R{\}}{\^{}}M{\$} which can be corrupted by an additive noise. Given the Encryption Redundancy Parameter (ERP) {\$}\backslashmu=M/N\backslashge 1{\$}, the signal strength parameter {\$}R=\backslashsqrt{\{}\backslashsum{\_}i s{\_}i{\^{}}2/N{\}}{\$}, and the ('bare') noise-to-signal ratio (NSR) {\$}\backslashgamma\backslashge 0{\$}, we consider the problem of reconstructing {\$}{\{}\backslashbf s{\}}{\$} from its corrupted image by a Least Square Scheme for a certain class of random Gaussian mappings. The problem is equivalent to finding the configuration of minimal energy in a certain version of spherical spin glass model, with squared Gaussian-distributed random potential. We use the Parisi replica symmetry breaking scheme to evaluate the mean overlap {\$}p{\_}{\{}\backslashinfty{\}}\backslashin [0,1]{\$} between the original signal and its recovered image (known as 'estimator') as {\$}N\backslashto \backslashinfty{\$}, which is a measure of the quality of the signal reconstruction. We explicitly analyze the general case of linear-quadratic family of random mappings and discuss the full {\$}p{\_}{\{}\backslashinfty{\}} (\backslashgamma){\$} curve. When nonlinearity exceeds a certain threshold but redundancy is not yet too big, the replica symmetric solution is necessarily broken in some interval of NSR. We show that encryptions with a nonvanishing linear component permit reconstructions with {\$}p{\_}{\{}\backslashinfty{\}}{\textgreater}0{\$} for any {\$}\backslashmu{\textgreater}1{\$} and any {\$}\backslashgamma{\textless}\backslashinfty{\$}, with {\$}p{\_}{\{}\backslashinfty{\}}\backslashsim \backslashgamma{\^{}}{\{}-1/2{\}}{\$} as {\$}\backslashgamma\backslashto \backslashinfty{\$}. In contrast, for the case of purely quadratic nonlinearity, for any ERP {\$}\backslashmu{\textgreater}1{\$} there exists a threshold NSR value {\$}\backslashgamma{\_}c(\backslashmu){\$} such that {\$}p{\_}{\{}\backslashinfty{\}}=0{\$} for {\$}\backslashgamma{\textgreater}\backslashgamma{\_}c(\backslashmu){\$} making the reconstruction impossible. The behaviour close to the threshold is given by {\$}p{\_}{\{}\backslashinfty{\}}\backslashsim (\backslashgamma{\_}c-\backslashgamma){\^{}}{\{}3/4{\}}{\$} and is controlled by the replica symmetry breaking mechanism.},
archivePrefix = {arXiv},
arxivId = {1805.06982},
author = {Fyodorov, Yan V},
eprint = {1805.06982},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1805.06982.pdf:pdf},
pages = {1--33},
title = {{A spin glass model for reconstructing nonlinearly encrypted signals corrupted by noise}},
url = {http://arxiv.org/abs/1805.06982},
year = {2018}
}
@article{Lesieur2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1701.00858v3},
author = {Lesieur, Thibault and Krzakala, Florent and Zdeborova, Lenka},
eprint = {arXiv:1701.00858v3},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1701.00858.pdf:pdf},
pages = {1--64},
title = {{Constrained Low-rank Matrix Estimation: Phase Transitions, Approximate Message Passing and Applications.}},
year = {2017}
}
@article{Urbani2013,
author = {Urbani, Pierfrancesco},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Urbani.pdf:pdf},
journal = {Thesis},
number = {1088567},
title = {{Theory of fluctuations in disordered systems}},
year = {2013}
}
@article{Obuchi2009a,
abstract = {The weight space of the Ising perceptron in which a set of random patterns is stored is examined using the generating function of the partition function {\$}\backslashphi(n)=(1/N)\backslashlog [Z{\^{}}n]{\$} as the dimension of the weight vector {\$}N{\$} tends to infinity, where {\$}Z{\$} is the partition function and {\$}[ ... ]{\$} represents the configurational average. We utilize {\$}\backslashphi(n){\$} for two purposes, depending on the value of the ratio {\$}\backslashalpha=M/N{\$}, where {\$}M{\$} is the number of random patterns. For {\$}\backslashalpha {\textless} \backslashalpha{\_}{\{}\backslashrm s{\}}=0.833 ...{\$}, we employ {\$}\backslashphi(n){\$}, in conjunction with Parisi's one-step replica symmetry breaking scheme in the limit of {\$}n \backslashto 0{\$}, to evaluate the complexity that characterizes the number of disjoint clusters of weights that are compatible with a given set of random patterns, which indicates that, in typical cases, the weight space is equally dominated by a single large cluster of exponentially many weights and exponentially many small clusters of a single weight. For {\$}\backslashalpha {\textgreater} \backslashalpha{\_}{\{}\backslashrm s{\}}{\$}, on the other hand, {\$}\backslashphi(n){\$} is used to assess the rate function of a small probability that a given set of random patterns is atypically separable by the Ising perceptrons. We show that the analyticity of the rate function changes at {\$}\backslashalpha = \backslashalpha{\_}{\{}\backslashrm GD{\}}=1.245 ... {\$}, which implies that the dominant configuration of the atypically separable patterns exhibits a phase transition at this critical ratio. Extensive numerical experiments are conducted to support the theoretical predictions.},
archivePrefix = {arXiv},
arxivId = {0910.2281},
author = {Obuchi, Tomoyuki and Kabashima, Yoshiyuki},
doi = {10.1088/1742-5468/2009/12/P12014},
eprint = {0910.2281},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1. Weight space structure and analysis using ad finite replica number in the Ising perceptron.pdf:pdf},
issn = {1742-5468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
month = {dec},
number = {12},
pages = {P12014},
title = {{Weight space structure and analysis using a finite replica number in the Ising perceptron}},
url = {http://stacks.iop.org/1742-5468/2009/i=12/a=P12014?key=crossref.a275d866de9c7571ee60474f8fca9af5},
volume = {2009},
year = {2009}
}
@article{Baldassi2015,
abstract = {We show that discrete synaptic weights can be efficiently used for learning in large scale neural systems, and lead to unanticipated computational performance.We focus on the representative case of learning random patterns with binary synapses in single layer networks. The standard statistical analysis shows that this problem is exponentially dominated by isolated solutions that are extremely hard to find algorithmically. Here,we introduce a novel method that allows us to find analytical evidence for the existence of subdominant and extremely dense regions of solutions. Numerical experiments confirm these findings.We also show that the dense regions are surprisingly accessible by simple learning protocols, and that these synaptic configurations are robust to perturbations and generalize better than typical solutions. These outcomes extend to synapses with multiple states and to deeper neural architectures. The large deviation measure also suggests how to design novel algorithmic schemes for optimization based on local entropy maximization.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.05753v1},
author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
doi = {10.1103/PhysRevLett.115.128101},
eprint = {arXiv:1509.05753v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/SM.pdf:pdf},
issn = {10797114},
journal = {Physical Review Letters},
number = {12},
pmid = {26431018},
title = {{Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses}},
volume = {115},
year = {2015}
}
@article{Martin2005a,
abstract = {The multi-index matching problem generalizes the well known matching problem by going from pairs to d -uplets. We use the cavity method from statistical physics to analyse its properties when the costs of the d -uplets are random. At low temperatures we find for d ≥ 3 a frozen glassy phase with vanishing entropy. We also investigate some properties of small samples by enumerating the lowest cost matchings to compare with our theoretical predictions.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0507180v1},
author = {Martin, O. C. and M{\'{e}}zard, M. and Rivoire, O.},
doi = {10.1088/1742-5468/2005/09/P09006},
eprint = {0507180v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0507180.pdf:pdf},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {Cavity and replica method,Typical-case computational complexity},
number = {9},
pages = {159--193},
primaryClass = {cond-mat},
title = {{Random multi-index matching problems}},
year = {2005}
}
@article{Cavagna2003,
abstract = {We revisit two classic Thouless-Anderson-Palmer (TAP) studies of the Sherrington-Kirkpatrick model [Bray A J and Moore M A 1980 J. Phys. C 13, L469; De Dominicis C and Young A P, 1983 J. Phys. A 16, 2063]. By using the Becchi-Rouet-Stora-Tyutin (BRST) supersymmetry, we prove the general equivalence of TAP and replica partition functions, and show that the annealed calculation of the TAP complexity is formally identical to the quenched thermodynamic calculation of the free energy at one step level of replica symmetry breaking. The complexity we obtain by means of the BRST symmetry turns out to be considerably smaller than the previous non-symmetric value.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0210665},
author = {Cavagna, Andrea and Giardina, Irene and Parisi, Giorgio and M{\'{e}}zard, Marc},
doi = {10.1088/0305-4470/36/5/301},
eprint = {0210665},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/3. On the formal equivalence of the TAP and thermodynamic methods in the SK model $\backslash$: Cavagna - Giardina - Parisi - M{\'{e}}zard.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {5},
pages = {1175--1194},
primaryClass = {cond-mat},
title = {{On the formal equivalence of the TAP and thermodynamic methods in the SK model}},
volume = {36},
year = {2003}
}
@article{Venturi2018,
abstract = {Neural networks provide a rich class of high-dimensional, non-convex optimization problems. Despite their non-convexity, gradient-descent methods often successfully optimize these models. This has motivated a recent spur in research attempting to characterize properties of their loss surface that may be responsible for such success. In particular, several authors have noted that over-parametrization appears to act as a remedy against non-convexity. In this paper, we address this phenomenon by studying key topological properties of the loss, such as the presence or absence of "spurious valleys", defined as connected components of sub-level sets that do not include a global minimum. Focusing on a class of two-layer neural networks defined by smooth (but generally non-linear) activation functions, our main contribution is to prove that as soon as the hidden layer size matches the intrinsic dimension of the reproducing space, defined as the linear functional space generated by the activations, no spurious valleys exist, thus allowing the existence of descent directions. Our setup includes smooth activations such as polynomials, both in the empirical and population risk, and generic activations in the empirical risk case.},
archivePrefix = {arXiv},
arxivId = {1802.06384},
author = {Venturi, Luca and Bandeira, Afonso S. and Bruna, Joan},
eprint = {1802.06384},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1802.06384.pdf:pdf},
pages = {1--25},
title = {{Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys}},
url = {http://arxiv.org/abs/1802.06384},
year = {2018}
}
@article{Wyart2003,
abstract = {We study Sutton's 'microcanonical' model for the internal organization of firms, that leads to non-trivial scaling properties for the statistics of growth rates. We show that the growth rates are asymptotically Gaussian in this model, whereas empirical results suggest that the kurtosis of the distribution increases with size. We also obtain the conditional distribution of the number and size of sub-sectors in Sutton's model. We formulate and solve an alternative model, based on the assumption that the sector sizes follow a power-law distribution. We find in this new model both anomalous scaling of the variance of growth rates and non-Gaussian asymptotic distributions. We give some testable predictions of the two models that would differentiate them further. We also discuss why the growth rate statistics at the country level and at the company level should be identical. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0210479},
author = {Wyart, Matthieu and Bouchaud, Jean Philippe},
doi = {10.1016/S0378-4371(03)00267-X},
eprint = {0210479},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0210479v2.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Corporate growth,Pareto distribution,Scaling,Size distribution},
number = {1-2},
pages = {241--255},
primaryClass = {cond-mat},
title = {{Statistical models for company growth}},
volume = {326},
year = {2003}
}
@article{Zdeborova2009,
abstract = {Optimization is fundamental in many areas of science, from computer science and information theory to engineering and statistical physics, as well as to biology or social sciences. It typically involves a large number of variables and a cost function depending on these variables. Optimization problems in the NP-complete class are particularly difficult, it is believed that the number of operations required to minimize the cost function is in the most difficult cases exponential in the system size. However, even in an NP-complete problem the practically arising instances might, in fact, be easy to solve. The principal question we address in this thesis is: How to recognize if an NP-complete constraint satisfaction problem is typically hard and what are the main reasons for this? We adopt approaches from the statistical physics of disordered systems, in particular the cavity method developed originally to describe glassy systems. We describe new properties of the space of solutions in two of the most studied constraint satisfaction problems - random satisfiability and random graph coloring. We suggest a relation between the existence of the so-called frozen variables and the algorithmic hardness of a problem. Based on these insights, we introduce a new class of problems which we named "locked" constraint satisfaction, where the statistical description is easily solvable, but from the algorithmic point of view they are even more challenging than the canonical satisfiability.},
archivePrefix = {arXiv},
arxivId = {0806.4112},
author = {Zdeborov{\'{a}}, Lenka},
doi = {10.2478/v10155-010-0096-6},
eprint = {0806.4112},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0806.4112.pdf:pdf},
issn = {03230465},
journal = {Acta Physica Slovaca},
keywords = {Average computational complexity,Belief propagation,Cavity method,Clustering of solutions,Constraint satisfaction problems,Random graphs coloring problem,Reconstruction on trees,Replica symmetry breaking,Satisfiability threshold,Spin glasses},
number = {3},
pages = {169--303},
title = {{Statistical physics of hard optimization problems}},
volume = {59},
year = {2009}
}
@article{Ricci-Tersenghi2018,
abstract = {Many inference problems, notably the stochastic block model (SBM) that generates a random graph with a hidden community structure, undergo phase transitions as a function of the signal-to-noise ratio, and can exhibit hard phases in which optimal inference is information-theoretically possible but computationally challenging. In this paper we refine this description in two ways. In a qualitative perspective we emphasize the existence of more generic phase diagrams with a hybrid-hard phase in which it is computationally easy to reach a non-trivial inference accuracy, but computationally hard to match the information theoretically optimal one. We support this discussion by quantitative expansions of the functional cavity equations that describe inference problems on sparse graphs. These expansions shed light on the existence of hybrid-hard phases, for a large class of planted constraint satisfaction problems, and on the question of the tightness of the Kesten-Stigum (KS) bound for the associated tree reconstruction problem. Our results show that the instability of the trivial fixed point is not a generic evidence for the Bayes-optimality of the message passing algorithms. We clarify in particular the status of the symmetric SBM with 4 communities and of the tree reconstruction of the associated Potts model: in the assortative (ferromagnetic) case the KS bound is always tight, whereas in the disassortative (antiferromagnetic) case we exhibit an explicit criterion involving the degree distribution that separates a large degree regime where the KS bound is tight and a low degree regime where it is not. We also investigate the SBM with 2 communities of different sizes, a.k.a. the asymmetric Ising model, and describe quantitatively its computational gap as a function of its asymmetry. We complement this study with numerical simulations of the Belief Propagation iterative algorithm.},
archivePrefix = {arXiv},
arxivId = {1806.11013},
author = {Ricci-Tersenghi, Federico and Semerjian, Guilhem and Zdeborova, Lenka},
eprint = {1806.11013},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1806.11013.pdf:pdf},
pages = {1--63},
title = {{Typology of phase transitions in Bayesian inference problems}},
url = {http://arxiv.org/abs/1806.11013},
year = {2018}
}
@article{Franz2017,
abstract = {Random constraint satisfaction problems (CSP) have been studied extensively using statistical physics techniques. They provide a benchmark to study average case scenarios instead of the worst case one. The interplay between statistical physics of disordered systems and computer science has brought new light into the realm of computational complexity theory, by introducing the notion of clustering of solutions, related to replica symmetry breaking. However, the class of problems in which clustering has been studied often involve discrete degrees of freedom: standard random CSPs are random K-SAT (aka disordered Ising models) or random coloring problems (aka disordered Potts models). In this work we consider instead problems that involve continuous degrees of freedom. The simplest prototype of these problems is the perceptron. Here we discuss in detail the full phase diagram of the model. In the regions of parameter space where the problem is non-convex, leading to multiple disconnected clusters of solutions, the solution is critical at the SAT/UNSAT threshold and lies in the same universality class of the jamming transition of soft spheres. We show how the critical behavior at the satisfiability threshold emerges, and we compute the critical exponents associated to the approach to the transition from both the SAT and UNSAT phase. We conjecture that there is a large universality class of non-convex continuous CSPs whose SAT-UNSAT threshold is described by the same scaling solution.},
archivePrefix = {arXiv},
arxivId = {1702.06919},
author = {Franz, Silvio and Parisi, Giorgio and Sevelev, Maksim and Urbani, Pierfrancesco and Zamponi, Francesco},
doi = {10.21468/SciPostPhys.2.3.019},
eprint = {1702.06919},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/33. Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems.pdf:pdf},
issn = {2542-4653},
pages = {1--28},
title = {{Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems}},
url = {http://arxiv.org/abs/1702.06919{\%}0Ahttp://dx.doi.org/10.21468/SciPostPhys.2.3.019},
year = {2017}
}
@article{Model1989,
author = {Model, Minimum Torque-change},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Bounds on the learning capacity of some multi-layer networks.pdf:pdf},
pages = {89--101},
title = {{Biological Cybernetics}},
volume = {101},
year = {1989}
}
@article{Manoel2017,
abstract = {We consider the problem of reconstructing a signal from multi-layered (possibly) non-linear measurements. Using non-rigorous but standard methods from statistical physics we present the Multi-Layer Approximate Message Passing (ML-AMP) algorithm for computing marginal probabilities of the corresponding estimation problem and derive the associated state evolution equations to analyze its performance. We also give the expression of the asymptotic free energy and the minimal information-theoretically achievable reconstruction error. Finally, we present some applications of this measurement model for compressed sensing and perceptron learning with structured matrices/patterns, and for a simple model of estimation of latent variables in an auto-encoder.},
archivePrefix = {arXiv},
arxivId = {1701.06981},
author = {Manoel, Andre and Krzakala, Florent and M{\'{e}}zard, Marc and Zdeborov{\'{a}}, Lenka},
doi = {10.1109/ISIT.2017.8006899},
eprint = {1701.06981},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1701.06981v1.pdf:pdf},
isbn = {9781509040964},
title = {{Multi-Layer Generalized Linear Estimation}},
url = {http://arxiv.org/abs/1701.06981{\%}0Ahttp://dx.doi.org/10.1109/ISIT.2017.8006899},
year = {2017}
}
@article{Kabashima2016,
abstract = {We analyse the matrix factorization problem. Given a noisy measurement of a product of two matrices, the problem is to estimate back the original matrices. It arises in many applications such as dictionary learning, blind matrix calibration, sparse principal component analysis, blind source separation, low rank matrix completion, robust principal component analysis or factor analysis. It is also important in machine learning: unsupervised representation learning can often be studied through matrix factorization. We use the tools of statistical mechanics - the cavity and replica methods - to analyze the achievability and computational tractability of the inference problems in the setting of Bayes-optimal inference, which amounts to assuming that the two matrices have random independent elements generated from some known distribution, and this information is available to the inference algorithm. In this setting, we compute the minimal mean-squared-error achievable in principle in any computational time, and the error that can be achieved by an efficient approximate message passing algorithm. The computation is based on the asymptotic state-evolution analysis of the algorithm. The performance that our analysis predicts, both in terms of the achieved mean-squared-error, and in terms of sample complexity, is extremely promising and motivating for a further development of the algorithm.},
archivePrefix = {arXiv},
arxivId = {1402.1298},
author = {Kabashima, Yoshiyuki and Krzakala, Florent and Mezard, Marc and Sakata, Ayaka and Zdeborova, Lenka},
doi = {10.1109/TIT.2016.2556702},
eprint = {1402.1298},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/30. Phase transitions and sample complexity in Bayes-optimal matrix factorization.pdf:pdf},
isbn = {1402.1298},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Statistical inference,computational barriers,dictionary learning,message passing algorithms,phase transitions,probabilistic matrix factorization,sparse coding,statistical and computational tradeoff,statistical physics},
number = {7},
pages = {4228--4265},
title = {{Phase Transitions and Sample Complexity in Bayes-Optimal Matrix Factorization}},
volume = {62},
year = {2016}
}
@article{Ros2018,
abstract = {We study rough high-dimensional landscapes in which an increasingly stronger preference for a given configuration emerges. Such energy landscapes arise in glass physics and inference. In particular we focus on random Gaussian functions, and on the spiked-tensor model and generalizations. We thoroughly analyze the statistical properties of the corresponding landscapes and characterize the associated geometrical phase transitions. In order to perform our study, we develop a framework based on the Kac-Rice method that allows to compute the complexity of the landscape, i.e. the logarithm of the typical number of stationary points and their Hessian. This approach generalizes the one used to compute rigorously the annealed complexity of mean-field glass models. We discuss its advantages with respect to previous frameworks, in particular the thermodynamical replica method which is shown to lead to partially incorrect predictions.},
archivePrefix = {arXiv},
arxivId = {1804.02686},
author = {Ros, Valentina and {Ben Arous}, Gerard and Biroli, Giulio and Cammarota, Chiara},
eprint = {1804.02686},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1804.02686.pdf:pdf},
title = {{Complex energy landscapes in spiked-tensor and simple glassy models: ruggedness, arrangements of local minima and phase transitions}},
url = {http://arxiv.org/abs/1804.02686},
year = {2018}
}
@article{Bouchaud2013a,
abstract = {Financial and economic history is strewn with bubbles and crashes, booms and busts, crises and upheavals of all sorts. Understanding the origin of these events is arguably one of the most important problems in economic theory. In this paper, we review recent efforts to include heterogeneities and interactions in models of decision. We argue that the Random Field Ising model (RFIM) indeed provides a unifying framework to account for many collective socio-economic phenomena that lead to sudden ruptures and crises. We discuss different models that can capture potentially destabilising self-referential feedback loops, induced either by herding, i.e. reference to peers, or trending, i.e. reference to the past, and account for some of the phenomenology missing in the standard models. We discuss some empirically testable predictions of these models, for example robust signatures of RFIM-like herding effects, or the logarithmic decay of spatial correlations of voting patterns. One of the most striking result, inspired by statistical physics methods, is that Adam Smith's invisible hand can badly fail at solving simple coordination problems. We also insist on the issue of time-scales, that can be extremely long in some cases, and prevent socially optimal equilibria to be reached. As a theoretical challenge, the study of so-called "detailed-balance" violating decision rules is needed to decide whether conclusions based on current models (that all assume detailed-balance) are indeed robust and generic.},
archivePrefix = {arXiv},
arxivId = {1209.0453},
author = {Bouchaud, Jean Philippe},
doi = {10.1007/s10955-012-0687-3},
eprint = {1209.0453},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Crises+and+collective+socio-economic+phenomena.pdf:pdf},
isbn = {0022-4715},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Avalanches,Collective phenomena,Crises,Random field Ising model},
number = {3-4},
pages = {567--606},
title = {{Crises and Collective Socio-Economic Phenomena: Simple Models and Challenges}},
volume = {151},
year = {2013}
}
@article{Kamilov2014,
archivePrefix = {arXiv},
arxivId = {1207.3859},
author = {Kamilov, Ulugbek S and Member, Student and Rangan, Sundeep and Fletcher, Alyson K and Unser, Michael},
eprint = {1207.3859},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1207.3859.pdf:pdf},
number = {5},
pages = {2969--2985},
title = {{Parameter Estimation and Applications to Sparse Learning}},
volume = {60},
year = {2014}
}
@article{Baldassi2016,
abstract = {In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost-function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare - but extremely dense and accessible - regions of configurations in the network weight space. We define a novel measure, which we call the "robust ensemble" (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models, and also provide a general algorithmic scheme which is straightforward to implement: define a cost-function given by a sum of a finite number of replicas of the original cost-function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful new algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems.},
archivePrefix = {arXiv},
arxivId = {1605.06444},
author = {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
doi = {10.1073/pnas.1608103113},
eprint = {1605.06444},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/E7655.full.pdf:pdf},
issn = {0027-8424},
pmid = {27856745},
title = {{Unreasonable Effectiveness of Learning Neural Networks: From Accessible States and Robust Ensembles to Basic Algorithmic Schemes}},
url = {http://arxiv.org/abs/1605.06444{\%}0Ahttp://dx.doi.org/10.1073/pnas.1608103113},
year = {2016}
}
@article{Marquardt2017,
author = {Marquardt, Florian},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/machine-learning-physicists.pdf:pdf},
title = {{Machine Learning for Physicists}},
year = {2017}
}
@article{Baity-Jesi2018,
abstract = {We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are (1) the complexity of the loss landscape and of the dynamics within it, and (2) to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and datasets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.},
archivePrefix = {arXiv},
arxivId = {1803.06969},
author = {Baity-Jesi, M. and Sagun, L. and Geiger, M. and Spigler, S. and {Ben Arous}, Gerard and Cammarota, C. and LeCun, Y. and Wyart, M. and Biroli, G.},
doi = {arXiv:1803.06969v2},
eprint = {1803.06969},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1803.06969.pdf:pdf},
title = {{Comparing Dynamics: Deep Neural Networks versus Glassy Systems}},
url = {http://arxiv.org/abs/1803.06969},
year = {2018}
}
@article{Majer1993a,
author = {Majer, P. and Engel, A. and Zippelius, A.},
doi = {10.1088/0305-4470/26/24/015},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/42. Percepton above saturation.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {24},
pages = {7405--7416},
title = {{Perceptrons above saturation}},
volume = {26},
year = {1993}
}
@article{Bouchaud2016,
author = {Bouchaud, M Jean-philippe and Saad, M David},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1610.04337.pdf:pdf},
title = {{de l ' Universit{\'{e}} de recherche Paris Sciences et Lettres Spectral Inference Methods on Sparse Graphs : Theory and Applications M{\'{e}}thodes spectrales d ' inf{\'{e}}rence sur les graphes parcimonieux Th{\'{e}}orie et applications Soutenue par Alaa Saade le 3 octobre 2016}},
year = {2016}
}
@article{Rattray1998,
abstract = {Natural gradient descent is an on-line variable-metric optimization algorithm which utilizes an underlying Riemannian parameter space. We analyze the dynamics of natural gradient descent beyond the asymptotic regime by employing an exact statistical mechanics description of learning in two-layer feed-forward neural networks. For a realizable learning scenario we find significant improvements over standard gradient descent for both the transient and asymptotic stages of learning, with a slower power law increase in learning time as task complexity grows. [S0031-9007(98)07950-2].},
author = {Rattray, M and Saad, D and Amari, S},
doi = {10.1103/PhysRevLett.81.5461},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.81.5461.pdf:pdf},
isbn = {0031-9007},
issn = {0031-9007},
journal = {Physical Review Letters},
keywords = {multilayer neural networks,online},
number = {24},
pages = {5461--5464},
title = {{Natural gradient descent for on-line learning}},
volume = {81},
year = {1998}
}
@article{Gualdi2017,
abstract = {We generalise the stylised macroeconomic Agent-Based model introduced in our previous paper ("Tipping points in macroeconomic agent-based models", JEDC 50 29–61 2015), with the aim of investigating the role and efficacy of monetary policy of a "Central Bank", that sets the interest rate such as to steer the economy towards a prescribed inflation and unemployment level. Our major finding is that provided its policy is not too aggressive (in a sense detailed in the paper) the Central Bank is successful in achieving its goals. However, the existence of different equilibrium states of the economy, separated by phase boundaries (or "dark corners"), can cause the monetary policy itself to trigger instabilities and be counter-productive. In other words, the Central Bank must navigate in a narrow window: too little is not enough, too much leads to instabilities and wildly oscillating economies. This conclusion strongly contrasts with the prediction of DSGE models. },
archivePrefix = {arXiv},
arxivId = {1501.00434},
author = {Gualdi, Stanislao and Tarzia, Marco and Zamponi, Francesco and Bouchaud, Jean Philippe},
doi = {10.1007/s11403-016-0174-z},
eprint = {1501.00434},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1501.00434v2.pdf:pdf},
issn = {18607128},
journal = {Journal of Economic Interaction and Coordination},
number = {3},
pages = {507--537},
title = {{Monetary policy and dark corners in a stylized agent-based model}},
volume = {12},
year = {2017}
}
@article{Miolane2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.00473v3},
author = {{Leo Miolane}},
eprint = {arXiv:1702.00473v3},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1702.00473.pdf:pdf},
pages = {1--34},
title = {{Fundamental limits of low-rank matrix estimation : the non-symmetric case}}
}
@article{Shinzato2009,
abstract = {Learning behavior of simple perceptrons is analyzed for a teacher-student scenario in which output labels are provided by a teacher network for a set of possibly correlated input patterns, and such that the teacher and student networks are of the same type. Our main concern is the effect of statistical correlations among the input patterns on learning performance. For this purpose, we extend to the teacher-student scenario a methodology for analyzing randomly labeled patterns recently developed in Shinzato and Kabashima 2008 J. Phys. A: Math. Theor. 41 324013. This methodology is used for analyzing situations in which orthogonality of the input patterns is enhanced in order to optimize the learning performance. {\textcopyright} 2009 IOP Publishing Ltd.},
archivePrefix = {arXiv},
arxivId = {arXiv:0809.1978v1},
author = {Shinzato, Takashi and Kabashima, Yoshiyuki},
doi = {10.1088/1751-8113/42/1/015005},
eprint = {arXiv:0809.1978v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Shinzato{\_}2009{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Theor.{\_}42{\_}015005.pdf:pdf},
issn = {17518113},
journal = {Journal of Physics A: Mathematical and Theoretical},
number = {1},
title = {{Learning from correlated patterns by simple perceptrons}},
volume = {42},
year = {2009}
}
@article{Krzakala2011,
abstract = {Compressed sensing is triggering a major evolution in signal acquisition. It consists in sampling a sparse signal at low rate and later using computational power for its exact reconstruction, so that only the necessary information is measured. Currently used reconstruction techniques are, however, limited to acquisition rates larger than the true density of the signal. We design a new procedure which is able to reconstruct exactly the signal with a number of measurements that approaches the theoretical limit in the limit of large systems. It is based on the joint use of three essential ingredients: a probabilistic approach to signal reconstruction, a message-passing algorithm adapted from belief propagation, and a careful design of the measurement matrix inspired from the theory of crystal nucleation. The performance of this new algorithm is analyzed by statistical physics methods. The obtained improvement is confirmed by numerical studies of several cases.},
archivePrefix = {arXiv},
arxivId = {1109.4424},
author = {Krzakala, Florent and M{\'{e}}zard, Marc and Sausset, Fran{\c{c}}ois and Sun, Yifan and Zdeborov{\'{a}}, Lenka},
doi = {10.1103/PhysRevX.2.021005},
eprint = {1109.4424},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1109.4424.pdf:pdf},
isbn = {2160-3308},
issn = {2160-3308},
pages = {1--21},
title = {{Statistical physics-based reconstruction in compressed sensing}},
url = {http://arxiv.org/abs/1109.4424{\%}0Ahttp://dx.doi.org/10.1103/PhysRevX.2.021005},
year = {2011}
}
@article{Zamponi2010,
abstract = {These lecture notes focus on the mean field theory of spin glasses, with particular emphasis on the presence of a very large number of metastable states in these systems. This phenomenon, and some of its physical consequences, will be discussed in details for fully-connected models and for models defined on random lattices. This will be done using the replica and cavity methods. These notes have been prepared for a course of the PhD program in Statistical Mechanics at SISSA, Trieste and at the University of Rome "Sapienza". Part of the material is reprinted from other lecture notes, and when this is done a reference is obviously provided to the original.},
archivePrefix = {arXiv},
arxivId = {1008.4844},
author = {Zamponi, Francesco},
eprint = {1008.4844},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1008.4844.pdf:pdf},
isbn = {1478643770823},
title = {{Mean field theory of spin glasses}},
url = {http://arxiv.org/abs/1008.4844},
year = {2010}
}
@article{Crisanti2015,
abstract = {The Replica Fourier Transform is the generalization of the discrete Fourier Transform to quantities defined on an ultrametric tree. It finds use in conjunction of the replica method used to study thermodynamics properties of disordered systems such as spin glasses. Its definition is presented in a systematic and simple form and its use illustrated with some representative examples. In particular we give a detailed discussion of the diagonalization in the Replica Fourier Space of the Hessian matrix of the Gaussian fluctuations about the mean field saddle point of spin glass theory. The general results are finally discussed for a generic spherical spin glass model, where the Hessian can be computed analytically.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.4023v1},
author = {Crisanti, A. and {De Dominicis}, C.},
doi = {10.1016/j.nuclphysb.2014.12.002},
eprint = {arXiv:1410.4023v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1410.4023.pdf:pdf},
issn = {05503213},
journal = {Nuclear Physics B},
keywords = {09,11 2014,18 ac,3,v 1},
pages = {73--105},
title = {{Replica Fourier Transform: Properties and applications}},
volume = {891},
year = {2015}
}
@article{Zdeborova2007,
abstract = {We consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. Using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions). We show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. First, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. Afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. Another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. Eventually, above the coloring threshold, no more solutions are available. We compute all the critical connectivities for Erdos-Renyi and regular random graphs and determine their asymptotic values for large number of colors. Finally, we discuss the algorithmic consequences of our findings. We argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. We also discuss the performance of a simple local Walk-COL algorithm and of the belief propagation algorithm in the light of our results.},
archivePrefix = {arXiv},
arxivId = {0704.1269},
author = {Zdeborov{\'{a}}, Lenka and Krza̧ka{\l}a, Florent},
doi = {10.1103/PhysRevE.76.031131},
eprint = {0704.1269},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/46. Phase transitions in the coloring graph.pdf:pdf},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {3},
pages = {1--37},
pmid = {17930223},
title = {{Phase transitions in the coloring of random graphs}},
volume = {76},
year = {2007}
}
@article{Barbier2017,
abstract = {We study the approximate message-passing decoder for sparse superposition coding on the additive white Gaussian noise channel and extend our preliminary work [1]. We use heuristic statistical-physics-based tools such as the cavity and the replica methods for the statistical analysis of the scheme. While superposition codes asymptotically reach the Shannon capacity, we show that our iterative decoder is limited by a phase transition similar to the one that happens in Low Density Parity check codes. We consider two solutions to this problem, that both allow to reach the Shannon capacity: i) a power allocation strategy and ii) the use of spatial coupling, a novelty for these codes that appears to be promising. We present in particular simulations suggesting that spatial coupling is more robust and allows for better reconstruction at finite code lengths. Finally, we show empirically that the use of a fast Hadamard-based operator allows for an efficient reconstruction, both in terms of computational time and memory, and the ability to deal with very large messages.},
archivePrefix = {arXiv},
arxivId = {1503.08040},
author = {Barbier, Jean and Krzakala, Florent},
doi = {10.1109/TIT.2017.2713833},
eprint = {1503.08040},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1503.08040.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Additive white Gaussian noise channel,Approximate message-passing,Capacity achieving,Compressed sensing,Error-correcting codes,Fast Hadamard operator,Power allocation,Replica analysis,Sparse superposition codes,Spatial coupling,State evolution},
number = {8},
pages = {4894--4927},
title = {{Approximate Message-Passing Decoder and Capacity Achieving Sparse Superposition Codes}},
volume = {63},
year = {2017}
}
@article{Rotskoff2018,
abstract = {Neural networks, a central tool in machine learning, have demonstrated remarkable, high fidelity performance on image recognition and classification tasks. These successes evince an ability to accurately represent high dimensional functions, potentially of great use in computational and applied mathematics. That said, there are few rigorous results about the representation error and trainability of neural networks. Here we characterize both the error and the scaling of the error with the size of the network by reinterpreting the standard optimization algorithm used in machine learning applications, stochastic gradient descent, as the evolution of a particle system with interactions governed by a potential related to the objective or "loss" function used to train the network. We show that, when the number {\$}n{\$} of parameters is large, the empirical distribution of the particles descends on a convex landscape towards a minimizer at a rate independent of {\$}n{\$}. We establish a Law of Large Numbers and a Central Limit Theorem for the empirical distribution, which together show that the approximation error of the network universally scales as {\$}O(n{\^{}}{\{}-1{\}}){\$}. Remarkably, these properties do not depend on the dimensionality of the domain of the function that we seek to represent. Our analysis also quantifies the scale and nature of the noise introduced by stochastic gradient descent and provides guidelines for the step size and batch size to use when training a neural network. We illustrate our findings on examples in which we train neural network to learn the energy function of the continuous 3-spin model on the sphere. The approximation error scales as our analysis predicts in as high a dimension as {\$}d=25{\$}.},
archivePrefix = {arXiv},
arxivId = {1805.00915},
author = {Rotskoff, Grant M. and Vanden-Eijnden, Eric},
eprint = {1805.00915},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1805.00915.pdf:pdf},
pages = {1--35},
title = {{Neural Networks as Interacting Particle Systems: Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Error}},
url = {http://arxiv.org/abs/1805.00915},
year = {2018}
}
@article{Saad1995,
author = {Saad, David and Solla, Sara A},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/56. On-line learning in soft committee machine.pdf:pdf},
number = {2},
pages = {3--4},
title = {{Online learning in soft committee machines}},
volume = {52},
year = {1995}
}
@article{Gabrie2017,
abstract = {We study in this paper the structure of solutions in the random hypergraph coloring problem and the phase transitions they undergo when the density of constraints is varied. Hypergraph coloring is a constraint satisfaction problem where each constraint includes {\$}K{\$} variables that must be assigned one out of {\$}q{\$} colors in such a way that there are no monochromatic constraints, i.e. there are at least two distinct colors in the set of variables belonging to every constraint. This problem generalizes naturally coloring of random graphs ({\$}K=2{\$}) and bicoloring of random hypergraphs ({\$}q=2{\$}), both of which were extensively studied in past works. The study of random hypergraph coloring gives us access to a case where both the size {\$}q{\$} of the domain of the variables and the arity {\$}K{\$} of the constraints can be varied at will. Our work provides explicit values and predictions for a number of phase transitions that were discovered in other constraint satisfaction problems but never evaluated before in hypergraph coloring. Among other cases we revisit the hypergraph bicoloring problem ({\$}q=2{\$}) where we find that for {\$}K=3{\$} and {\$}K=4{\$} the colorability threshold is not given by the one-step-replica-symmetry-breaking analysis as the latter is unstable towards more levels of replica symmetry breaking. We also unveil and discuss the coexistence of two different 1RSB solutions in the case of {\$}q=2{\$}, {\$}K \backslashge 4{\$}. Finally we present asymptotic expansions for the density of constraints at which various phase transitions occur, in the limit where {\$}q{\$} and/or {\$}K{\$} diverge.},
archivePrefix = {arXiv},
arxivId = {1707.01983},
author = {Gabri{\'{e}}, Marylou and Dani, Varsha and Semerjian, Guilhem and Zdeborov{\'{a}}, Lenka},
doi = {10.1088/1751-8121/aa9529},
eprint = {1707.01983},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1707.01983.pdf:pdf},
issn = {17518121},
journal = {Journal of Physics A: Mathematical and Theoretical},
keywords = {cavity method,constraint satisfaction problem,disordered systems,message passing algorithms,phase transitions,random graphs},
number = {50},
title = {{Phase transitions in the q-coloring of random hypergraphs}},
volume = {50},
year = {2017}
}
@article{Kabashima1994,
author = {Kabashima, Y.},
doi = {10.1088/0305-4470/27/6/017},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/36. Perfect loss of generalization due to nise in K=2 parity machines.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {6},
pages = {1917--1927},
title = {{Perfect loss of generalization due to noise in K=2 parity machines}},
volume = {27},
year = {1994}
}
@article{Obuchi2009b,
abstract = {The weight space of the Ising perceptron in which a set of random patterns is stored is examined using the generating function of the partition function {\$}\backslashphi(n)=(1/N)\backslashlog [Z{\^{}}n]{\$} as the dimension of the weight vector {\$}N{\$} tends to infinity, where {\$}Z{\$} is the partition function and {\$}[ ... ]{\$} represents the configurational average. We utilize {\$}\backslashphi(n){\$} for two purposes, depending on the value of the ratio {\$}\backslashalpha=M/N{\$}, where {\$}M{\$} is the number of random patterns. For {\$}\backslashalpha {\textless} \backslashalpha{\_}{\{}\backslashrm s{\}}=0.833 ...{\$}, we employ {\$}\backslashphi(n){\$}, in conjunction with Parisi's one-step replica symmetry breaking scheme in the limit of {\$}n \backslashto 0{\$}, to evaluate the complexity that characterizes the number of disjoint clusters of weights that are compatible with a given set of random patterns, which indicates that, in typical cases, the weight space is equally dominated by a single large cluster of exponentially many weights and exponentially many small clusters of a single weight. For {\$}\backslashalpha {\textgreater} \backslashalpha{\_}{\{}\backslashrm s{\}}{\$}, on the other hand, {\$}\backslashphi(n){\$} is used to assess the rate function of a small probability that a given set of random patterns is atypically separable by the Ising perceptrons. We show that the analyticity of the rate function changes at {\$}\backslashalpha = \backslashalpha{\_}{\{}\backslashrm GD{\}}=1.245 ... {\$}, which implies that the dominant configuration of the atypically separable patterns exhibits a phase transition at this critical ratio. Extensive numerical experiments are conducted to support the theoretical predictions.},
archivePrefix = {arXiv},
arxivId = {0910.2281},
author = {Obuchi, Tomoyuki and Kabashima, Yoshiyuki},
doi = {10.1088/1742-5468/2009/12/P12014},
eprint = {0910.2281},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Obuchi{\_}2009{\_}J.{\_}Stat.{\_}Mech.{\_}2009{\_}P12014.pdf:pdf},
issn = {1742-5468},
title = {{Weight space structure and analysis using a finite replica number in the Ising perceptron}},
url = {http://arxiv.org/abs/0910.2281{\%}0Ahttp://dx.doi.org/10.1088/1742-5468/2009/12/P12014},
year = {2009}
}
@article{Castellani2005,
abstract = {In these notes the main theoretical concepts and techniques in the field of mean-field spin-glasses are reviewed in a compact and pedagogical way, for the benefit of the graduate and undergraduate student. One particular spin-glass model is analyzed (the p-spin spherical model) by using three different approaches. Thermodynamics, covering pure states, overlaps, overlap distribution, replica symmetry breaking, and the static transition. Dynamics, covering the generating functional method, generalized Langevin equation, equations for the correlation and the response, the Mode Coupling approximation, and the dynamical transition. And finally complexity, covering the mean-field (TAP) free energy, metastable states, entropy crisis, threshold energy, and saddles. Particular attention has been paid on the mutual consistency of the results obtained from the different methods.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0505032},
author = {Castellani, Tommaso and Cavagna, Andrea},
doi = {10.1088/1742-5468/2005/05/P05012},
eprint = {0505032},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0505032.pdf:pdf},
isbn = {1742-5468},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {Cavity and replica method,Disordered systems (theory),Slow dynamics and ageing (theory),Spin glasses (theory)},
number = {5},
pages = {215--266},
pmid = {229586200004},
primaryClass = {cond-mat},
title = {{Spin-glass theory for pedestrians}},
year = {2005}
}
@article{Safran2017,
abstract = {We consider the optimization problem associated with training simple ReLU neural networks of the form {\$}\backslashmathbf{\{}x{\}}\backslashmapsto \backslashsum{\_}{\{}i=1{\}}{\^{}}{\{}k{\}}\backslashmax\backslash{\{}0,\backslashmathbf{\{}w{\}}{\_}i{\^{}}\backslashtop \backslashmathbf{\{}x{\}}\backslash{\}}{\$} with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is unrestricted, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once {\$}k\backslashgeq 6{\$}. By a continuity argument, this implies that in high dimensions, $\backslash$emph{\{}nearly all{\}} target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.},
archivePrefix = {arXiv},
arxivId = {1712.08968},
author = {Safran, Itay and Shamir, Ohad},
eprint = {1712.08968},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1712.08968.pdf:pdf},
number = {1},
title = {{Spurious Local Minima are Common in Two-Layer ReLU Neural Networks}},
url = {http://arxiv.org/abs/1712.08968},
year = {2017}
}
@article{Panchenko2012,
abstract = {The goal of this paper is to review some of the main ideas that emerged from the attempts to confirm mathematically the predictions of the celebrated Parisi ansatz in the Sherrington-Kirkpatrick model. We try to focus on the big picture while sketching the proofs of only a few selected results, but an interested reader can find most of the missing details in [31] and [44].},
archivePrefix = {arXiv},
arxivId = {1211.1094},
author = {Panchenko, Dmitry},
doi = {10.1007/s10955-012-0586-7},
eprint = {1211.1094},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1211.1094.pdf:pdf},
keywords = {2010,60k35,82b44,mathematics subject classification,parisi ansatz,sherrington-kirkpatrick model},
number = {2},
pages = {1--25},
title = {{The Sherrington-Kirkpatrick model: an overview}},
url = {http://arxiv.org/abs/1211.1094{\%}0Ahttp://dx.doi.org/10.1007/s10955-012-0586-7},
volume = {2},
year = {2012}
}
@article{Mezard2017,
abstract = {Motivated by recent progress in using restricted Boltzmann machines as preprocessing algorithms for deep neural network, we revisit the mean-field equations (belief-propagation and TAP equations) in the best understood such machine, namely the Hopfield model of neural networks, and we explicit how they can be used as iterative message-passing algorithms, providing a fast method to compute the local polarizations of neurons. In the "retrieval phase" where neurons polarize in the direction of one memorized pattern, we point out a major difference between the belief propagation and TAP equations : the set of belief propagation equations depends on the pattern which is retrieved, while one can use a unique set of TAP equations. This makes the latter method much better suited for applications in the learning process of restricted Boltzmann machines. In the case where the patterns memorized in the Hopfield model are not independent, but are correlated through a combinatorial structure, we show that the TAP equations have to be modified. This modification can be seen either as an alteration of the reaction term in TAP equations, or, more interestingly, as the consequence of message passing on a graphical model with several hidden layers, where the number of hidden layers depends on the depth of the correlations in the memorized patterns. This layered structure is actually necessary when one deals with more general restricted Boltzmann machines.},
archivePrefix = {arXiv},
arxivId = {1608.01558},
author = {M{\'{e}}zard, Marc},
doi = {10.1103/PhysRevE.95.022117},
eprint = {1608.01558},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevE.95.022117.pdf:pdf},
issn = {24700053},
journal = {Physical Review E},
number = {2},
pages = {1--15},
title = {{Mean-field message-passing equations in the Hopfield model and its generalizations}},
volume = {95},
year = {2017}
}
@article{Cherrier2003,
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0211695v1},
author = {Cherrier, R. and Dean, D. S. and Lef{\`{e}}vre, A.},
doi = {10.1103/PhysRevE.67.046112},
eprint = {0211695v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0211695.pdf:pdf},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {4},
pages = {10},
primaryClass = {arXiv:cond-mat},
title = {{Role of the interaction matrix in mean-field spin glass models}},
volume = {67},
year = {2003}
}
@article{Rattray1999,
abstract = {Natural gradient descent is a principled method for adapting the parameters of a statistical model on-line using an underlying Riemannian parameter space to redefine the direction of steepest descent. The algorithm is examined via methods of statistical physics which accurately characterize both transient and asymptotic behavior. A solution of the learning dynamics is obtained for the case of multilayer neural network training in the limit of large input dimension. We find that natural gradient learning leads to optimal asymptotic performance and outperforms gradient descent in the transient, significantly shortening or even removing plateaus in the transient generalization performance which typically hamper gradient descent training. 87.10.+e, 02.50.-r, 05.20.-y},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/9901212v1},
author = {Rattray, Magnus and Saad, David},
eprint = {9901212v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevE.59.4523.pdf:pdf},
number = {4},
pages = {4523--4532},
primaryClass = {arXiv:cond-mat},
title = {{Analysis of Natural Gradient Descent for Multilayer Neural Networks}},
volume = {59},
year = {1999}
}
@article{Giudice1989,
author = {Giudice, P Del and Franz, S and Virasoro, M A},
doi = {10.1051/jphys:01989005002012100},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/ajp-jphys{\_}1989{\_}50{\_}2{\_}121{\_}0.pdf:pdf},
isbn = {Preprint No. 612},
issn = {0302-0738},
journal = {J. Phys. France},
number = {2},
pages = {121--134},
title = {{Perceptron beyond the limit of capacity}},
volume = {50},
year = {1989}
}
@article{Schwaeze1992,
author = {Schwaeze, H. and Hertz, J.},
doi = {10.1209/0295-5075/20/4/015},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/44. Generalization in a large committee machine.pdf:pdf},
issn = {12864854},
journal = {Epl},
number = {4},
pages = {375--380},
title = {{Generalization in a large committee machine}},
volume = {20},
year = {1992}
}
@article{Gauvin2010,
author = {Gauvin, Laetitia},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/These.pdf:pdf},
number = {Paris VI},
title = {{Mod{\'{e}}lisation de syst{\`{e}}mes socio-{\'{e}}conomiques {\`{a}} l ' aide des outils de physique statistique}},
year = {2010}
}
@article{Chung2017a,
abstract = {Perceptual manifolds arise when a neural population responds to an ensemble of sensory signals associated with different physical features (e.g., orientation, pose, scale, location, and intensity) of the same perceptual object. Object recognition and discrimination requires classifying the manifolds in a manner that is insensitive to variability within a manifold. How neuronal systems give rise to invariant object classification and recognition is a fundamental problem in brain theory as well as in machine learning. Here we study the ability of a readout network to classify objects from their perceptual manifold representations. We develop a statistical mechanical theory for the linear classification of manifolds with arbitrary geometry revealing a remarkable relation to the mathematics of conic decomposition. Novel geometrical measures of manifold radius and manifold dimension are introduced which can explain the classification capacity for manifolds of various geometries. The general theory is demonstrated on a number of representative manifolds, including L2 ellipsoids prototypical of strictly convex manifolds, L1 balls representing polytopes consisting of finite sample points, and orientation manifolds which arise from neurons tuned to respond to a continuous angle variable, such as object orientation. The effects of label sparsity on the classification capacity of manifolds are elucidated, revealing a scaling relation between label sparsity and manifold radius. Theoretical predictions are corroborated by numerical simulations using recently developed algorithms to compute maximum margin solutions for manifold dichotomies. Our theory and its extensions provide a powerful and rich framework for applying statistical mechanics of linear classification to data arising from neuronal responses to object stimuli, as well as to artificial deep networks trained for object recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1710.06487},
author = {Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
doi = {10.1103/PhysRevX.8.031003},
eprint = {1710.06487},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1710.06487(2).pdf:pdf},
issn = {2160-3308},
pages = {1--24},
title = {{Classification and Geometry of General Perceptual Manifolds}},
url = {http://arxiv.org/abs/1710.06487{\%}0Ahttp://dx.doi.org/10.1103/PhysRevX.8.031003},
year = {2017}
}
@article{Hansel1992,
abstract = {The supervised learning of a rule that can be realized by a multilayer network (the teacher) functioning as a parity machine with K = 2 hidden units and nonoverlapping receptive fields is studied. The student network is supposed to have the same architecture as the teacher. Application of statistical mechanics shows that when the number of examples is smaller than a critical value P* the trained network is unable to generalize the rule from the examples. Numerical simulations exhibiting this phenomenon are discussed.},
author = {Hansel, D. and Mato, G. and Meunier, C.},
doi = {10.1209/0295-5075/20/5/015},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/54. Memorization Without Generalization in a Multilayered Neural Network.pdf:pdf},
issn = {12864854},
journal = {Epl},
number = {5},
pages = {471--476},
title = {{Memorization without generalization in a multilayered neural network}},
volume = {20},
year = {1992}
}
@article{Crisanti2005,
abstract = {The Complexity of the Thouless-Anderson-Palmer (TAP) solutions of the Ising p-spin is investigated in the temperature regime where the equilibrium phase is one step Replica Symmetry Breaking. Two solutions of the resulting saddle point equations are found. One is supersymmetric (SUSY) and includes the equilibrium value of the free energy while the other is non-SUSY. The two solutions cross exactly at a value of the free energy where the replicon eigenvalue is zero; at low free energy the complexity is described by the SUSY solution while at high free energy it is described by the non-SUSY solution. In particular the non-SUSY solution describes the total number of solutions, like in the Sherrington-Kirkpatrick (SK) model. The relevant TAP solutions corresponding to the non-SUSY solution share the same feature of the corresponding solutions in the SK model, in particular their Hessian has a vanishing isolated eigenvalue. The TAP solutions corresponding to the SUSY solution, instead, are well separated minima.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0406649},
author = {Crisanti, A. and Leuzzi, L. and Rizzo, T.},
doi = {10.1103/PhysRevB.71.094202},
eprint = {0406649},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevB.71.094202.pdf:pdf},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {9},
pages = {1--11},
primaryClass = {cond-mat},
title = {{Complexity in mean-field spin-glass models: Ising p-spin}},
volume = {71},
year = {2005}
}
@article{Whyte1996,
author = {Whyte, W. and Sherrington, D.},
doi = {10.1088/0305-4470/29/12/014},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/43. RSB in perceptrons.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {12},
pages = {3063--3073},
title = {{Replica-symmetry breaking in perceptrons}},
volume = {29},
year = {1996}
}
@article{Yaida2018,
abstract = {The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.},
archivePrefix = {arXiv},
arxivId = {1810.00004},
author = {Yaida, Sho},
doi = {10.1515/cog-2013-0031},
eprint = {1810.00004},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1810.00004.pdf:pdf},
isbn = {1810.00004v1},
issn = {09365907},
pages = {1--12},
title = {{Fluctuation-dissipation relations for stochastic gradient descent}},
url = {http://arxiv.org/abs/1810.00004},
year = {2018}
}
@article{Chung2017b,
abstract = {We consider the problem of classifying data manifolds where each manifold represents invariances that are parameterized by continuous degrees of freedom. Conventional data augmentation methods rely upon sampling large numbers of training examples from these manifolds; instead, we propose an iterative algorithm called M{\_}{\{}CP{\}} based upon a cutting-plane approach that efficiently solves a quadratic semi-infinite programming problem to find the maximum margin solution. We provide a proof of convergence as well as a polynomial bound on the number of iterations required for a desired tolerance in the objective function. The efficiency and performance of M{\_}{\{}CP{\}} are demonstrated in high-dimensional simulations and on image manifolds generated from the ImageNet dataset. Our results indicate that M{\_}{\{}CP{\}} is able to rapidly learn good classifiers and shows superior generalization performance compared with conventional maximum margin methods using data augmentation methods.},
archivePrefix = {arXiv},
arxivId = {1705.09944},
author = {Chung, SueYeon and Cohen, Uri and Sompolinsky, Haim and Lee, Daniel D.},
eprint = {1705.09944},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1705.09944.pdf:pdf},
title = {{Learning Data Manifolds with a Cutting Plane Method}},
url = {http://arxiv.org/abs/1705.09944},
year = {2017}
}
@article{Gardner1988a,
author = {Gardner, E},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/E{\_}Gardner{\_}1988{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Gen.{\_}21{\_}030.pdf:pdf},
journal = {J. Phys. A},
pages = {257},
title = {{The space of interations in neural network models}},
volume = {21},
year = {1988}
}
@article{Barbier2017b,
abstract = {We consider generalized linear models (GLMs) where an unknown {\$}n{\$}-dimensional signal vector is observed through the application of a random matrix and a non-linear (possibly probabilistic) componentwise output function. We consider the models in the high-dimensional limit, where the observation consists of m points, and {\$}m/n\backslashto\backslashalpha{\$} where {\$}\backslashalpha{\$} stays finite in the limit {\$}m,n\backslashto\backslashinfty{\$}. This situation is ubiquitous in applications ranging from supervised machine learning to signal processing. A substantial amount of theoretical work analyzed the model-case when the observation matrix has i.i.d. elements and the components of the ground-truth signal are taken independently from some known distribution. While statistical physics provided number of explicit conjectures for special cases of this model, results existing for non-linear output functions were so far non-rigorous. At the same time GLMs with non-linear output functions are used as a basic building block of powerful multilayer feedforward neural networks. Therefore rigorously establishing the formulas conjectured for the mutual information is a key open problem that we solve in this paper. We also provide an explicit asymptotic formula for the optimal generalization error, and confirm the prediction of phase transitions in GLMs. Analyzing the resulting formulas for several non-linear output functions, including the rectified linear unit or modulus functions, we obtain quantitative descriptions of information-theoretic limitations of high-dimensional inference. Our proof technique relies on a new version of the interpolation method with an adaptive interpolation path and is of independent interest. Furthermore we show that a polynomial-time algorithm referred to as generalized approximate message-passing reaches the optimal generalization error for a large set of parameters.},
archivePrefix = {arXiv},
arxivId = {1708.03395},
author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'{e}}o and Zdeborov{\'{a}}, Lenka},
eprint = {1708.03395},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Barbier et al. - 2017 - Phase Transitions, Optimal Errors and Optimality of Message-Passing in Generalized Linear Models.pdf:pdf},
pages = {1--59},
title = {{Phase Transitions, Optimal Errors and Optimality of Message-Passing in Generalized Linear Models}},
url = {http://arxiv.org/abs/1708.03395},
year = {2017}
}
@article{MezardMarc;N1984,
author = {{Mezard, Marc ; N}, Soulas; G. Toulouse; M. Virasoro},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/84{\_}MPSTV{\_}JDP.pdf:pdf},
isbn = {0198400450508},
pages = {843--854},
title = {{Replica symmetry breaking and the nature of the spin glass phase}},
volume = {206},
year = {1984}
}
@article{Martin2005,
abstract = {The multi-index matching problem generalizes the well known matching problem by going from pairs to d -uplets. We use the cavity method from statistical physics to analyse its properties when the costs of the d -uplets are random. At low temperatures we find for d ≥ 3 a frozen glassy phase with vanishing entropy. We also investigate some properties of small samples by enumerating the lowest cost matchings to compare with our theoretical predictions.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0507180v1},
author = {Martin, O. C. and M{\'{e}}zard, M. and Rivoire, O.},
doi = {10.1088/1742-5468/2005/09/P09006},
eprint = {0507180v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0507180-2.pdf:pdf},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {Cavity and replica method,Typical-case computational complexity},
number = {9},
pages = {159--193},
primaryClass = {cond-mat},
title = {{Random multi-index matching problems}},
year = {2005}
}
@article{Mezard2017a,
abstract = {Motivated by recent progress in using restricted Boltzmann machines as preprocessing algorithms for deep neural network, we revisit the mean-field equations (belief-propagation and TAP equations) in the best understood such machine, namely the Hopfield model of neural networks, and we explicit how they can be used as iterative message-passing algorithms, providing a fast method to compute the local polarizations of neurons. In the "retrieval phase" where neurons polarize in the direction of one memorized pattern, we point out a major difference between the belief propagation and TAP equations : the set of belief propagation equations depends on the pattern which is retrieved, while one can use a unique set of TAP equations. This makes the latter method much better suited for applications in the learning process of restricted Boltzmann machines. In the case where the patterns memorized in the Hopfield model are not independent, but are correlated through a combinatorial structure, we show that the TAP equations have to be modified. This modification can be seen either as an alteration of the reaction term in TAP equations, or, more interestingly, as the consequence of message passing on a graphical model with several hidden layers, where the number of hidden layers depends on the depth of the correlations in the memorized patterns. This layered structure is actually necessary when one deals with more general restricted Boltzmann machines.},
archivePrefix = {arXiv},
arxivId = {1608.01558},
author = {M{\'{e}}zard, Marc},
doi = {10.1103/PhysRevE.95.022117},
eprint = {1608.01558},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1608.01558.pdf:pdf},
issn = {24700053},
journal = {Physical Review E},
number = {2},
title = {{Mean-field message-passing equations in the Hopfield model and its generalizations}},
volume = {95},
year = {2017}
}
@article{Gardner1989,
abstract = {The optimal storage properties of three different neural network models$\backslash$nare studied. For two of these models the architecture of the network$\backslash$nis a perceptron with +or-J interactions, whereas for the third model$\backslash$nthe output can be an arbitrary function of the inputs. Analytic bounds$\backslash$nand numerical estimates of the optimal capacities and of the minimal$\backslash$nfraction of errors are obtained for the first two models. The third$\backslash$nmodel can be solved exactly and the exact solution is compared to$\backslash$nthe bounds and to the results of numerical simulations used for the$\backslash$ntwo other models.},
author = {Gardner, E. and Derrida, B.},
doi = {10.1088/0305-4470/22/12/004},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/3unfinished.pdf:pdf},
issn = {13616447},
journal = {Journal of Physics A: Mathematical and General},
number = {12},
pages = {1983--1994},
title = {{Three unfinished works on the optimal storage capacity of networks}},
volume = {22},
year = {1989}
}
@article{Baldassi2015b,
abstract = {We show that discrete synaptic weights can be efficiently used for learning in large scale neural systems, and lead to unanticipated computational performance.We focus on the representative case of learning random patterns with binary synapses in single layer networks. The standard statistical analysis shows that this problem is exponentially dominated by isolated solutions that are extremely hard to find algorithmically. Here,we introduce a novel method that allows us to find analytical evidence for the existence of subdominant and extremely dense regions of solutions. Numerical experiments confirm these findings.We also show that the dense regions are surprisingly accessible by simple learning protocols, and that these synaptic configurations are robust to perturbations and generalize better than typical solutions. These outcomes extend to synapses with multiple states and to deeper neural architectures. The large deviation measure also suggests how to design novel algorithmic schemes for optimization based on local entropy maximization.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.05753v1},
author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
doi = {10.1103/PhysRevLett.115.128101},
eprint = {arXiv:1509.05753v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.115.128101.pdf:pdf},
issn = {10797114},
journal = {Physical Review Letters},
number = {12},
pages = {1--5},
pmid = {26431018},
title = {{Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses}},
volume = {115},
year = {2015}
}
@article{Alemi2017,
abstract = {A fundamental aspect of limitations in learning any computation in neural architectures is characterizing their optimal capacities. An important, widely-used neural architecture is known as autoencoders where the network reconstructs the input at the output layer via a representation at a hidden layer. Even though capacities of several neural architectures have been addressed using statistical physics methods, the capacity of autoencoder neural networks is not well-explored. Here, we analytically show that an autoencoder network of binary neurons with a hidden layer can achieve a capacity that grows exponentially with network size. The network has fixed random weights encoding a set of dense input patterns into a dense, expanded (or $\backslash$emph{\{}overcomplete{\}}) hidden layer representation. A set of learnable weights decodes the input patters at the output layer. We perform a mean-field approximation of the model to reduce the model to a perceptron problem with an input-output dependency. Carrying out Gardner's $\backslash$emph{\{}replica{\}} calculation, we show that as the expansion ratio, defined as the number of hidden units over the number of input units, increases, the autoencoding capacity grows exponentially even when the sparseness or the coding level of the hidden layer representation is changed. The replica-symmetric solution is locally stable and is in good agreement with simulation results obtained using a local learning rule. In addition, the degree of symmetry between the encoding and decoding weights monotonically increases with the expansion ratio.},
archivePrefix = {arXiv},
arxivId = {1705.07441},
author = {Alemi, Alireza and Abbara, Alia},
eprint = {1705.07441},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1705.07441.pdf:pdf},
title = {{Exponential Capacity in an Autoencoder Neural Network with a Hidden Layer}},
url = {http://arxiv.org/abs/1705.07441},
year = {2017}
}
@article{Mezard1986,
abstract = {We introduce a new method, which does not use replicas, from which we recover all the results of the replica symmetry-breaking solution of the Sherrington-Kirkpatrick model.},
author = {M{\'{e}}zard, M. and Parisi, G. and Virasoro, M. A.},
doi = {10.1209/0295-5075/1/2/006},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/6. The SK Model Replicas solution without replicas.pdf:pdf},
issn = {12864854},
journal = {Epl},
number = {2},
pages = {77--82},
title = {{Sk model: The replica solution without replicas}},
volume = {1},
year = {1986}
}
@article{Mezard2003,
abstract = {In this note we explain the use of the cavity method directly at zero temperature, in the case of the spin glass on a Bethe lattice. The computation is done explicitly in the formalism equivalent to 'one step replica symmetry breaking'; we compute the energy of the global ground state, as well as the complexity of equilibrium states at a given energy. Full results are presented for a Bethe lattice with connectivity equal to three.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0207121},
author = {M{\'{e}}zard, Marc and Parisi, Giorgio},
doi = {10.1023/A:1022221005097},
eprint = {0207121},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0207121.pdf:pdf},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Bethe lattice,Cavity method,Spin glass},
number = {1-2},
pages = {1--34},
pmid = {180806300001},
primaryClass = {cond-mat},
title = {{The Cavity Method at Zero Temperature}},
volume = {111},
year = {2003}
}
@article{Hosaka2002,
abstract = {The performance of a lossy data compression scheme for uniformly biased Boolean messages is investigated via methods of statistical mechanics. Inspired by a formal similarity to the storage capacity problem in neural network research, we utilize a perceptron of which the transfer function is appropriately designed in order to compress and decode the messages. Employing the replica method, we analytically show that our scheme can achieve the optimal performance known in the framework of lossy compression in most cases when the code length becomes infinite. The validity of the obtained results is numerically confirmed.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0207356},
author = {Hosaka, Tadaaki and Kabashima, Yoshiyuki and Nishimori, Hidetoshi},
doi = {10.1103/PhysRevE.66.066126},
eprint = {0207356},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevE.66.066126.pdf:pdf},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {6},
pages = {8},
pmid = {12513366},
primaryClass = {cond-mat},
title = {{Statistical mechanics of lossy data compression using a nonmonotonic perceptron}},
volume = {66},
year = {2002}
}
@article{Crisanti1992,
abstract = {The relaxational dynamics for local spin autocor- relations of the spherical p-spin interaction spin-glass model is studied in the mean field limit. In the high temperature and high external field regime, the dynamics is ergodic and simi- lar to the behaviour in known liquid-glass transition models. In the static limit, we recover the replica symmetric solution for the long time correlation. This phase becomes unstable on a critical line in the (T, h) plane, where critical slowing down is observed with a cross-over to power law decay of the correlation function o( t-{\~{}}, with an exponent u varying along the critical line. For low temperatures and low fields, ergodicity in phase space is broken. For small fields the tran- sition is discontinuous, and approaching this transition from above, two tong time scales are seen to emerge. This dynam- ical transition lies at a somewhat higher temperature than the one obtained within replica theory. For larger fields the tran- sition becomes continuous at some tricritical point. The low temperature phase with broken ergodicity is studied within a modified equilibrium theory and alternatively for adiabatic cooling across the transition line. This latter scheme yields rather detailed insight into the formation and structure of the ergodic components.},
author = {Crisanti, A. and Sommers, H. J.},
doi = {10.1007/BF01309287},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/10.1007-BF01309287.pdf:pdf},
issn = {07223277},
journal = {Zeitschrift f{\"{u}}r Physik B Condensed Matter},
number = {3},
pages = {341--354},
title = {{The spherical p-spin interaction spin glass model: the statics}},
volume = {87},
year = {1992}
}
@article{Zdeborova2016,
abstract = {Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic.},
archivePrefix = {arXiv},
arxivId = {1511.02476},
author = {Zdeborov{\'{a}}, Lenka and Krzakala, Florent},
doi = {10.1080/00018732.2016.1211393},
eprint = {1511.02476},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Statistical physics of inference thresholds and algorithms.pdf:pdf},
isbn = {1511.02476},
issn = {14606976},
journal = {Advances in Physics},
keywords = {Bayesian inference,belief propagation,compressed sensing,phase transitions in computer science,spin glass theory,stochastic block model},
number = {5},
pages = {453--552},
title = {{Statistical physics of inference: thresholds and algorithms}},
volume = {65},
year = {2016}
}
@article{Watkin1993a,
abstract = {A summary is presented of the statistical mechanical theory of learning a rule with a neural network, a rapidly advancing area which is closely related to other inverse problems frequently encountered by physicists. By emphasizing the relationship between neural networks and strongly interacting physical systems, such as spin glasses, the authors show how learning theory has provided a workshop in which to develop new,exact analytical techniques.},
author = {Watkin, Timothy L H and Rau, Albrecht and Biehl, Michael},
doi = {10.1103/RevModPhys.65.499},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/RevModPhys.65.499.pdf:pdf},
issn = {0034-6861},
journal = {Rev. Mod. Phys.},
pages = {499--556},
title = {{The statistical mechanics of learning a rule}},
url = {http://link.aps.org/doi/10.1103/RevModPhys.65.499},
volume = {65},
year = {1993}
}
@article{Higgins2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.02230v1},
author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
eprint = {arXiv:1812.02230v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Towards a Deﬁnition of Disentangled Representations.pdf:pdf},
pages = {1--29},
title = {{Towards a Definition of Disentangled Representations}},
year = {2018}
}
@article{Advani2017a,
abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant "high-dimensional" regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
archivePrefix = {arXiv},
arxivId = {1710.03667},
author = {Advani, Madhu S. and Saxe, Andrew M.},
eprint = {1710.03667},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/14. High dimensional dynamics of the generalization error.pdf:pdf},
title = {{High-dimensional dynamics of generalization error in neural networks}},
url = {http://arxiv.org/abs/1710.03667},
year = {2017}
}
@article{Ros2018a,
abstract = {We analyze the energy barriers that allow escapes from a given local minimum in a mean-field model of glasses. We perform this study by using the Kac-Rice method and computing the typical number of critical points of the energy function at a given distance from the minimum. We analyze their Hessian in terms of random matrix theory and show that for a certain regime of energies and distances critical points are index-one saddles and are associated to barriers. We find that the lowest barrier, important for activated dynamics at low temperature, is strictly lower than the "threshold" level above which saddles proliferate. We characterize how the quenched complexity of barriers, important for activated process at finite temperature, depends on the energy of the barrier, the energy of the initial minimum, and the distance between them. The overall picture gained from this study is expected to hold generically for mean-field models of the glass transition.},
archivePrefix = {arXiv},
arxivId = {1809.05440},
author = {Ros, Valentina and Biroli, Giulio and Cammarota, Chiara},
eprint = {1809.05440},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1809.05440.pdf:pdf},
title = {{Complexity of energy barriers in mean-field glassy systems}},
url = {http://arxiv.org/abs/1809.05440},
year = {2018}
}
@article{Louart2017,
abstract = {This article studies the Gram random matrix model {\$}G=\backslashfrac1T\backslashSigma{\^{}}{\{}\backslashrm T{\}}\backslashSigma{\$}, {\$}\backslashSigma=\backslashsigma(WX){\$}, classically found in the analysis of random feature maps and random neural networks, where {\$}X=[x{\_}1,\backslashldots,x{\_}T]\backslashin{\{}\backslashmathbb R{\}}{\^{}}{\{}p\backslashtimes T{\}}{\$} is a (data) matrix of bounded norm, {\$}W\backslashin{\{}\backslashmathbb R{\}}{\^{}}{\{}n\backslashtimes p{\}}{\$} is a matrix of independent zero-mean unit variance entries, and {\$}\backslashsigma:{\{}\backslashmathbb R{\}}\backslashto{\{}\backslashmathbb R{\}}{\$} is a Lipschitz continuous (activation) function --- {\$}\backslashsigma(WX){\$} being understood entry-wise. By means of a key concentration of measure lemma arising from non-asymptotic random matrix arguments, we prove that, as {\$}n,p,T{\$} grow large at the same rate, the resolvent {\$}Q=(G+\backslashgamma I{\_}T){\^{}}{\{}-1{\}}{\$}, for {\$}\backslashgamma{\textgreater}0{\$}, has a similar behavior as that met in sample covariance matrix models, involving notably the moment {\$}\backslashPhi=\backslashfrac{\{}T{\}}n{\{}\backslashmathbb E{\}}[G]{\$}, which provides in passing a deterministic equivalent for the empirical spectral measure of {\$}G{\$}. Application-wise, this result enables the estimation of the asymptotic performance of single-layer random neural networks. This in turn provides practical insights into the underlying mechanisms into play in random neural networks, entailing several unexpected consequences, as well as a fast practical means to tune the network hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1702.05419},
author = {Louart, Cosme and Liao, Zhenyu and Couillet, Romain},
eprint = {1702.05419},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1702.05419.pdf:pdf},
keywords = {60B20, 62M45},
title = {{A Random Matrix Approach to Neural Networks}},
url = {http://arxiv.org/abs/1702.05419},
year = {2017}
}
@misc{Mezard1987,
abstract = {"The authors give a masterly account of the work on the Sherrington-Kirkpatrick model in the first 85 pages. Such a summary cannot be found elsewhere, and much of our current understanding of the problem is due to the authors of this book and their collaborators".This is an important book, which every physics library should have. It is also good reading for anyone who wants to know about these aspects of the spin glass problem, and why "replica symmetry breaking'' and "ultrametricity'' may be important".Physics Today, 1988" a very useful source of relevant information on the progress achieved by theoretical physicists in the field. It can be helpful also to mathematicians trying to develop a mathematically rigorous counterpart of this theory".EMS, 1999},
author = {Mezard, Marc and Parisi, Giorgio and Virasoro, Miguel Angel},
doi = {10.1063/1.2811676},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Spin Glass Theory and Beyond.djvu:djvu},
isbn = {9971501155},
issn = {00319228},
pages = {461},
pmid = {437123},
title = {{Spin Glass Theory and Beyond}},
year = {1987}
}
@article{Manoel2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1701.06981v1},
author = {Manoel, Andre and Marc, M and Zdeborova, Lenka},
eprint = {arXiv:1701.06981v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/22. ML-AMP.pdf:pdf},
pages = {1--5},
title = {{Multi-Layer Generalized Linear Estimation}},
year = {2017}
}
@article{Franz2001,
abstract = {We introduce a finite-connectivity ferromagnetic model with a three-spin interaction which has a crystalline (ferromagnetic) phase as well as a glass phase. The model is not frustrated, it has a ferromagnetic equilibrium phase at low temperature which is not reached dynamically in a quench from the high-temperature phase. Instead it shows a glass transition which can be studied in detail by a one step replica-symmetry broken calculation. This spin model exhibits the main properties of the structural glass transition at a solvable mean-field level.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0103026},
author = {Franz, S. and M{\'{e}}zard, M. and Ricci-Tersenghi, F. and Weigt, M. and Zecchina, R.},
doi = {10.1209/epl/i2001-00438-4},
eprint = {0103026},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/01{\_}FMRTWZ{\_}EPL.pdf:pdf},
issn = {02955075},
journal = {Europhysics Letters},
number = {4},
pages = {465--471},
primaryClass = {cond-mat},
title = {{A ferromagnet with a glass transition}},
volume = {55},
year = {2001}
}
@article{Shinzato2008,
abstract = {In this paper, we address the problem of how many randomly labeled patterns$\backslash$ncan be correctly classified by a single-layer perceptron when the patterns are$\backslash$ncorrelated with each other. In order to solve this problem, two analytical$\backslash$nschemes are developed based on the replica method and Thouless-Anderson-Palmer$\backslash$n(TAP) approach by utilizing an integral formula concerning random rectangular$\backslash$nmatrices. The validity and relevance of the developed methodologies are shown$\backslash$nfor one known result and two example problems. A message-passing algorithm to$\backslash$nperform the TAP scheme is also presented.},
archivePrefix = {arXiv},
arxivId = {0712.4050v1},
author = {Shinzato, Takashi and Kabashima, Yoshiyuki},
doi = {10.1088/1751-8113/41/32/324013},
eprint = {0712.4050v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Shinzato{\_}2008{\_}J.{\_}Phys.{\_}A{\%}3A{\_}Math.{\_}Theor.{\_}41{\_}324013.pdf:pdf},
issn = {17518113},
journal = {Journal of Physics A: Mathematical and Theoretical},
number = {32},
title = {{Perceptron capacity revisited: Classification ability for correlated patterns}},
volume = {41},
year = {2008}
}
@article{Gabrie2018,
abstract = {We examine a class of deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.},
archivePrefix = {arXiv},
arxivId = {1805.09785},
author = {Gabri{\'{e}}, Marylou and Manoel, Andre and Luneau, Cl{\'{e}}ment and Barbier, Jean and Macris, Nicolas and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
doi = {arXiv:1805.09785v1},
eprint = {1805.09785},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1805.09785.pdf:pdf},
title = {{Entropy and mutual information in models of deep neural networks}},
url = {http://arxiv.org/abs/1805.09785},
year = {2018}
}
@article{Gualdi2015a,
abstract = {The aim of this work is to explore the possible types of phenomena that simple macroeconomic Agent-Based models (ABMs) can reproduce. We propose a methodology, inspired by statistical physics, that characterizes a model through its "phase diagram" in the space of parameters. Our first motivation is to understand the large macro-economic fluctuations observed in the "Mark I" ABM devised by Delli Gatti and collaborators. In this regard, our major finding is the generic existence of a phase transition between a "good economy" where unemployment is low, and a "bad economy" where unemployment is high. We then introduce a simpler framework that allows us to show that this transition is robust against many modifications of the model, and is generically induced by an asymmetry between the rate of hiring and the rate of firing of the firms. The unemployment level remains small until a tipping point, beyond which the economy suddenly collapses. If the parameters are such that the system is close to this transition, any small fluctuation is amplified as the system jumps between the two equilibria. We have explored several natural extensions of the model. One is to introduce a bankruptcy threshold, limiting the firms maximum level of debt-to-sales ratio. This leads to a rich phase diagram with, in particular, a region where acute endogenous crises occur, during which the unemployment rate shoots up before the economy can recover. We also introduce simple wage policies. This leads to inflation (in the "good" phase) or deflation (in the "bad" phase), but leaves the overall phase diagram of the model essentially unchanged. We have also explored the effect of simple monetary policies that attempt to contain rising unemployment and defang crises. We end the paper with general comments on the usefulness of ABMs to model macroeconomic phenomena, in particular in view of the time needed to reach a steady state that raises the issue of ergodicity in these models.},
archivePrefix = {arXiv},
arxivId = {1307.5319},
author = {Gualdi, Stanislao and Tarzia, Marco and Zamponi, Francesco and Bouchaud, Jean Philippe},
doi = {10.1016/j.jedc.2014.08.003},
eprint = {1307.5319},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1307.5319v4.pdf:pdf},
issn = {01651889},
journal = {Journal of Economic Dynamics and Control},
keywords = {Agent-based computational economics,Aggregative models,Cycles},
pages = {29--61},
title = {{Tipping points in macroeconomic agent-based models}},
volume = {50},
year = {2015}
}
@article{Baldassi2015a,
abstract = {We show that discrete synaptic weights can be efficiently used for learning in large scale neural systems, and lead to unanticipated computational performance.We focus on the representative case of learning random patterns with binary synapses in single layer networks. The standard statistical analysis shows that this problem is exponentially dominated by isolated solutions that are extremely hard to find algorithmically. Here,we introduce a novel method that allows us to find analytical evidence for the existence of subdominant and extremely dense regions of solutions. Numerical experiments confirm these findings.We also show that the dense regions are surprisingly accessible by simple learning protocols, and that these synaptic configurations are robust to perturbations and generalize better than typical solutions. These outcomes extend to synapses with multiple states and to deeper neural architectures. The large deviation measure also suggests how to design novel algorithmic schemes for optimization based on local entropy maximization.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.05753v1},
author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
doi = {10.1103/PhysRevLett.115.128101},
eprint = {arXiv:1509.05753v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1509.05753.pdf:pdf},
issn = {10797114},
journal = {Physical Review Letters},
number = {12},
pages = {1--11},
pmid = {26431018},
title = {{Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses}},
volume = {115},
year = {2015}
}
@article{Bouten1994,
abstract = {It is demonstrated that the replica symmetric saddle point is unstable$\backslash$nwhen the distribution of aligned fields displays a gap.},
author = {Bouten, M.},
doi = {10.1088/0305-4470/27/17/033},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/32. Replica symmetry instability in perceptron models.pdf:pdf},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {17},
pages = {6021--6023},
title = {{Replica symmetry instability in perceptron models}},
volume = {27},
year = {1994}
}
@article{Manoel2017a,
author = {Manoel, Andre},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Andre.pdf:pdf},
title = {{Inference in Multi-Layer Graphical Models}},
year = {2017}
}
@article{Biroli2018,
abstract = {In this paper and in the companion one we address the problem of identifying the effective theory that describes the statistics of the fluctuations of what is thought to be the relevant order parameter for glassy systems---the overlap field with an equilibrium reference configuration---close to the putative thermodynamic glass transition. Our starting point is the mean-field theory of glass formation which relies on the existence of a complex free-energy landscape with a multitude of metastable states. In this paper, we focus on archetypal mean-field models possessing this type of free-energy landscape and set up the framework to determine the exact effective theory. We show that the effective theory at the mean-field level is generically of the random-field + random-bond Ising type. We also discuss what are the main issues concerning the extension of our result to finite-dimensional systems. This extension is addressed in detail in the companion paper.},
archivePrefix = {arXiv},
arxivId = {1807.06293},
author = {Biroli, G. and Cammarota, C. and Tarjus, G. and Tarzia, M.},
eprint = {1807.06293},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1807.06293.pdf:pdf},
title = {{Random-Field Ising like effective theory of the glass transition: I Mean-Field Models}},
url = {http://arxiv.org/abs/1807.06293},
year = {2018}
}
@article{Krzakala2012a,
abstract = {Compressed sensing is a signal processing method that acquires data directly in a compressed form. This allows one to make less measurements than what was considered necessary to record a signal, enabling faster or more precise measurement protocols in a wide range of applications. Using an interdisciplinary approach, we have recently proposed in [arXiv:1109.4424] a strategy that allows compressed sensing to be performed at acquisition rates approaching to the theoretical optimal limits. In this paper, we give a more thorough presentation of our approach, and introduce many new results. We present the probabilistic approach to reconstruction and discuss its optimality and robustness. We detail the derivation of the message passing algorithm for reconstruction and expectation max- imization learning of signal-model parameters. We further develop the asymptotic analysis of the corresponding phase diagrams with and without measurement noise, for different distribution of signals, and discuss the best possible reconstruction performances regardless of the algorithm. We also present new efficient seeding matrices, test them on synthetic data and analyze their performance asymptotically.},
archivePrefix = {arXiv},
arxivId = {1206.3953},
author = {Krzakala, Florent and M{\'{e}}zard, Marc and Sausset, Francois and Sun, Yifan and Zdeborov{\'{a}}, Lenka},
doi = {10.1088/1742-5468/2012/08/P08009},
eprint = {1206.3953},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/publijournalstatmech.pdf:pdf},
isbn = {1742-5468},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {cavity and replica method,error correcting codes,message-passing algorithms,statistical inference},
number = {8},
title = {{Probabilistic reconstruction in compressed sensing: Algorithms, phase diagrams, and threshold achieving matrices}},
volume = {2012},
year = {2012}
}
@article{Hosaka2002a,
abstract = {The performance of a lossy data compression scheme for uniformly biased Boolean messages is investigated via methods of statistical mechanics. Inspired by a formal similarity to the storage capacity problem in neural network research, we utilize a perceptron of which the transfer function is appropriately designed in order to compress and decode the messages. Employing the replica method, we analytically show that our scheme can achieve the optimal performance known in the framework of lossy compression in most cases when the code length becomes infinite. The validity of the obtained results is numerically confirmed.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0207356},
author = {Hosaka, Tadaaki and Kabashima, Yoshiyuki and Nishimori, Hidetoshi},
doi = {10.1103/PhysRevE.66.066126},
eprint = {0207356},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/0207356.pdf:pdf},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {6},
pages = {8},
pmid = {12513366},
primaryClass = {cond-mat},
title = {{Statistical mechanics of lossy data compression using a nonmonotonic perceptron}},
volume = {66},
year = {2002}
}
@article{Sompolinsky1990,
abstract = {A statistical mechanical theory of learning from examples in layered networks at finite temperature is studied. When the training error is a smooth function of continuously varying weights the generalization error falls off asymptotically as the inverse number of examples. By analytical and numerical studies of single-layer perceptrons we show that when the weights are discrete the generalization error can exhibit a discontinuous transition to perfect generalization. For intermediate sizes of the example set, the state of perfect generalization coexists with a metastable spin-glass state. PACS numbers: 87. 10.+e, 02.50.+s, 05.20.-y Understanding how systems can be efficiently trained to perform tasks is of fundamental importance. A cen-tral issue in learning theory is the rate of improvement in the processing of novel data as a function of the number of examples presented during training, i.e. , the generali-zation curve. ' Numerical results on training in layered neural networks indicate that the generalization error improves gradually in some cases, and sharply in oth-ers. s In this work we use statistical mechanics to study generalization curves in large layered networks. We will first discuss the general theory and then present results for learning in a single-layer perceptron. The computational function of layered neural net-works is described in terms of the input-output relations that they generate. We consider here a multilayer net-work with M input nodes, whose states are denoted by S;, i =I, . . . , M, and a single output node denoted by cr=cr(W;S) where the W{\~{}}, i = I, . . . , N, denote the synaptic weights of the network. The network is trained by adjusting its weights to approximate or reproduce, if possible, a target function cto(S) on the input space. This is achieved by providing a set of examples consist-ing of P input-output pairs (S', cro({\$}')), /= I, . . . , P. We assume that the inputs {\$}' are chosen at random from the entire input space. The training process is often described as the minimi-zation of a training energy P E(W) = g e(W;S'), where e(W;S) is some measure of the deviation of the network output o(W;S) from the target output cro({\$}), e. g., e(w;S) =fcr(W;S) — o'o(S)] . We consider a sto chastic training process that leads at long times to a Gibbs distribution of networks P(W) =Z 'exp[ PE(w)], — where Z— : fdWexp[ PE(W)], dW= + dW, P— o(W), ((Z")) = Q d W exp(PG [W ]), — o=l where (2) G = — ln dSexp — P g e(W;S) tT= l (3) The variables W represent the weights of n copies (re-plicas) of the system. The importance of the form (2) lies in the fact that the effective (nonrandom) Hamil-tonian G is intensive, and does not depend on the number of examples P. A direct consequence is that the correct thermodynamic limit (N {\~{}}) is achieved when the number of examples scales as the total number of (in-dependently determined) weights, i.e. , P = a{\~{A}}. This scaling guarantees that both the entropy and the energy and Po(W) is the a priori measure on the weight space. The temperature T=P ' denotes the level of stochastic noise in the training. In the limit T 0 the training corresponds to finding the global minimum of E. The training energy E depends on the fixed random choice of examples S'. Focusing on average quantities, we will first average over P(W) with fixed S'. This thermal average will be denoted by ()T. We will then perform a quenched average, i.e. , average over the distribution of example sets, (()) — = fgt dS'. Here dS represents the normalized measure over the space of inputs. The average training error is e, (T, P)= P— x(((E(W))r)). The performance of a network on the whole input space is measured by the generalization er-ror, defined as e(W) =fdSc(W;S) The . average gen-eralization error, after training with P examples, is cg(T, P)— = (((e(w))T)). The deviations of the typical values of these quantities from their thermal and quenched averages are expected to vanish as N We apply the replica method' to evaluate quenched averages. The averge free energy F is given by — PF= — ((lnZ)) = lim n '(((Z")) — 1) . n 0 Using Eq. (I) we find 1683},
author = {Sompolinsky, H. and Tishby, N. and Seung, H. S.},
doi = {10.1103/PhysRevLett.65.1683},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.65.1683.pdf:pdf},
isbn = {1079-7114 (Electronic)$\backslash$r0031-9007 (Linking)},
issn = {00319007},
journal = {Physical Review Letters},
number = {13},
pages = {1683--1686},
pmid = {10042332},
title = {{Learning from examples in large neural networks}},
volume = {65},
year = {1990}
}
@article{Sarkar2018,
abstract = {We consider the problem of jointly recovering the vector {\$}\backslashboldsymbol{\{}b{\}}{\$} and the matrix {\$}\backslashboldsymbol{\{}C{\}}{\$} from noisy measurements {\$}\backslashboldsymbol{\{}Y{\}} = \backslashboldsymbol{\{}A{\}}(\backslashboldsymbol{\{}b{\}})\backslashboldsymbol{\{}C{\}} + \backslashboldsymbol{\{}W{\}}{\$}, where {\$}\backslashboldsymbol{\{}A{\}}(\backslashcdot){\$} is a known affine linear function of {\$}\backslashboldsymbol{\{}b{\}}{\$} (i.e., {\$}\backslashboldsymbol{\{}A{\}}(\backslashboldsymbol{\{}b{\}})=\backslashboldsymbol{\{}A{\}}{\_}0+\backslashsum{\_}{\{}i=1{\}}{\^{}}Q b{\_}i \backslashboldsymbol{\{}A{\}}{\_}i{\$} with known matrices {\$}\backslashboldsymbol{\{}A{\}}{\_}i{\$}). This problem has applications in matrix completion, robust PCA, dictionary learning, self-calibration, blind deconvolution, joint-channel/symbol estimation, compressive sensing with matrix uncertainty, and many other tasks. To solve this bilinear recovery problem, we propose the Bilinear Adaptive Vector Approximate Message Passing (BAd-VAMP) algorithm. We demonstrate numerically that the proposed approach is competitive with other state-of-the-art approaches to bilinear recovery, including lifted VAMP and Bilinear GAMP.},
archivePrefix = {arXiv},
arxivId = {1809.00024},
author = {Sarkar, Subrata and Fletcher, Alyson K. and Rangan, Sundeep and Schniter, Philip},
eprint = {1809.00024},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/Bilinear Recovery using Adaptive Vector-AMP.pdf:pdf},
number = {2},
pages = {1--12},
title = {{Bilinear Recovery using Adaptive Vector-AMP}},
url = {http://arxiv.org/abs/1809.00024},
year = {2018}
}
@article{RemiMonassonandRiccardo1996,
author = {{R{\'{e}}mi, Monasson and Riccardo}, Zecchina},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/48. Weight space structure and representations{\_}2.pdf:pdf},
number = {6},
pages = {9007},
title = {{Weight Space Structure and Internal Representations : A Direct Approach to Learning and Generalization in Multilayer Neural Networks}},
volume = {76},
year = {1996}
}
@article{Rattray1999a,
abstract = {Natural gradient descent is a principled method for adapting the parameters of a statistical model on-line using an underlying Riemannian parameter space to redefine the direction of steepest descent. The algorithm is examined via methods of statistical physics which accurately characterize both transient and asymptotic behavior. A solution of the learning dynamics is obtained for the case of multilayer neural network training in the limit of large input dimension. We find that natural gradient learning leads to optimal asymptotic performance and outperforms gradient descent in the transient, significantly shortening or even removing plateaus in the transient generalization performance which typically hamper gradient descent training. 87.10.+e, 02.50.-r, 05.20.-y},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/9901212v1},
author = {Rattray, Magnus and Saad, David},
eprint = {9901212v1},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevE.59.4523.pdf:pdf},
number = {4},
pages = {4523--4532},
primaryClass = {arXiv:cond-mat},
title = {{Analysis of Natural Gradient Descent for Multilayer Neural Networks}},
volume = {59},
year = {1999}
}
@article{Mei2018a,
abstract = {Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties? In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearlyideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.},
archivePrefix = {arXiv},
arxivId = {1804.06561},
author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
eprint = {1804.06561},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/1804.06561.pdf:pdf},
title = {{A Mean Field View of the Landscape of Two-Layers Neural Networks}},
url = {http://arxiv.org/abs/1804.06561},
year = {2018}
}
@article{Rattray1998a,
abstract = {Natural gradient descent is an on-line variable-metric optimization algorithm which utilizes an underlying Riemannian parameter space. We analyze the dynamics of natural gradient descent beyond the asymptotic regime by employing an exact statistical mechanics description of learning in two-layer feed-forward neural networks. For a realizable learning scenario we find significant improvements over standard gradient descent for both the transient and asymptotic stages of learning, with a slower power law increase in learning time as task complexity grows. [S0031-9007(98)07950-2].},
author = {Rattray, M and Saad, D and Amari, S},
doi = {10.1103/PhysRevLett.81.5461},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.81.5461.pdf:pdf},
isbn = {0031-9007},
issn = {0031-9007},
journal = {Physical Review Letters},
keywords = {multilayer neural networks,online},
number = {24},
pages = {5461--5464},
title = {{Natural gradient descent for on-line learning}},
volume = {81},
year = {1998}
}
@article{Saad1995c,
author = {Saad, David and Solla, Sara A},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/56. On-line learning in soft committee machine.pdf:pdf},
number = {2},
pages = {3--4},
title = {{( F ) = Μ}},
volume = {52},
year = {1995}
}
@article{Saad1995b,
abstract = {We present an analytic solution to the problem of on-line gradient-descent learning for two-layer neural networks with an arbitrary number of hidden units in both teacher and student networks. PACS numbers: 87. 10.+e, 02.50. — r, 05.20. — y Layered neural networks are of interest for their abil-ity to implement input-output maps [1]. Classification and regression tasks formulated as a map from an N-dimensional input space g onto a scalar g are real-ized through a map g = fi(g), which can be modified through changes in the internal parameters [J) specifying the strength of the interneuron couplings [2,3]. Learn-ing refers to the modification of these couplings so as to bring the map f{\~{}} implemented by the network as close as possible to a desired map f. The degree of success is monitored through the generalization error, a measure of the dissimilarity between f{\~{}} and f. Learning from examples in layered neural networks is usually formulated as an optimization problem [2,3], based on the minimization of an additive learning er-ror defined over a training set composed of P inde-pendent examples ((t', ("), with (t' = f(g"), 1 {\~{}} p {\~{}} P. Statistical physics tools for investigating the prop-erties of such models, based on the use of the replica method, have been successfully applied to the analysis of single-layer perceptrons [3] and some simplified two-layer structures (e.g., committee machines [4]). Analysis of more complicated multilayer networks is hampered by technical difficulties due to the complex structure of the solutions in a space of order parameters [5], which de-scribe in this case correlations among the various neurons in the trained network, as well as their degree of special-ization toward the implementation of the desired task. A recently introduced alternative approach investigates on line learning -[6]. In this scenario the couplings are adjusted to minimize the error after the presentation of each example. The resulting changes in [J] are de-scribed as a dynamical evolution, with the number of examples playing the role of time. The average that ac-counts for the disorder introduced by the independent random selection of an example at each time step can be performed directly, without invoking the replica method. The resulting equations of motion for the relevant order parameters characterize the structure of the space of so-lutions and allow for a computation of the generalization error. While investigating the on-line learning scenario pro-posed by Biehl and Schwarze [6], we found an unexpected result: The dynamical equations for the order parameters can be obtained analytically for a general two-layer stu-dent network composed of N input units, K hidden units, and a single linear output unit, trained to perform a task defined through a teacher network of similar architecture, except that its number M of hidden units is not necessarily equal to K. Two-layer networks with an arbitrary number of hidden units have been shown to be universal approximators [1] for N-to-one dimensional maps. Our results thus describe the learning of tasks of arbitrary complexity (general M). The complexity of the student network is also arbitrary (general K, independent of M), providing a tool to investigate realizable (K = M), overrealizable (K) M), and unrealizable (K (M) learning scenarios. Such capabilities are to be contrasted with previously available results; the equations provided in [6] can only describe a committee rnachine with K = 2 hidden units learning a linearly separable task (M = 1). In this Letter we limit our discussion to the case of the soft-committee machine [6], in which all the hid-den units are connected to the output unit with positive couplings of unit strength, and only the input-to-hidden couplings are adaptive. Consider the student network: hid-den unit i receives information from input unit r through the weight J; " and its activation under presentation of an input pattern g = (gi, . . . , g{\~{}}) is x; = J; g, with J; = (J;i, . . . , J,{\~{}}) defined as the vector of incoming weights onto the ith hidden unit. The output of the student network is tr(J, g) = g, , g(J; g), where g is the activation func-tion of the hidden units, taken here to be the error function g(x) = — erf (x/{\~{}}2), and J — = [J;)i; tc is the set of input-to-hidden adaptive weights. Training examples are of the form (gt', gt'). The components of the independently drawn input vectors g{\&} are uncorrelated random variables with zero mean and unit variance. The corresponding output gP is given by a deterministic teacher whose internal structure is that of a network similar to the student except for a possible difference in the number M of hidden units. Hidden unit n in the teacher network receives input information through the weight vector B, = (B " i, . . . , B " tv), and its activation under presentation of the input pattern g{\&} is yn The corresponding output is g{\&} = g " ,g(B, gt").},
author = {Saad, David and Solla, Sara A.},
doi = {10.1103/PhysRevLett.74.4337},
file = {:Users/benjaminaubin/Documents/Bibliography Mendeley/PhysRevLett.74.4337.pdf:pdf},
issn = {00319007},
journal = {Physical Review Letters},
number = {21},
pages = {4337--4340},
title = {{Exact solution for on-line learning in multilayer neural networks}},
volume = {74},
year = {1995}
}
